<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Vision</title>
    <description>My personal blog for studying; mainly related to data
</description>
    <link>http://subong0508.github.io/</link>
    <atom:link href="http://subong0508.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 27 Jan 2021 12:41:01 +0900</pubDate>
    <lastBuildDate>Wed, 27 Jan 2021 12:41:01 +0900</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>프로그래머스 Lv1 추천문제 + 풀이 모음</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;추천문제&lt;/h2&gt;

&lt;p&gt;사실 코딩 수업을 들은 적이 있거나 독학한 적이 있으면 프로그래머스 Lv1은 손쉽게 풀 수 있다. 그리고 그 와중에서도 난이도 차이가 꽤나 많이 나서 추천문제만 모아보았다. 추천 문제들은 주로 구현 문제 또는 엣지 케이스에 잘 걸린다거나 효율성 테스트(시간복잡도 측정)에 걸리는 문제들이다. 그냥 내기준 다른 문제들보다 까다로운 문제들로 골라봤다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/64061&quot;&gt;크레인 인형뽑기 게임&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/72410&quot;&gt;신규 아이디 추천&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42862&quot;&gt;체육복&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12901&quot;&gt;2016년&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/68935&quot;&gt;3진법 뒤집기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12906&quot;&gt;같은 숫자는 싫어&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12915&quot;&gt;문자열 내 마음대로 정렬하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12921&quot;&gt;소수 찾기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12926&quot;&gt;시저 암호&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12930&quot;&gt;이상한 문자 만들기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/67256&quot;&gt;키패드 누르기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12982&quot;&gt;예산&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/17681&quot;&gt;[1차] 비밀지도&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42889&quot;&gt;실패율&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/17682&quot;&gt;[1차] 다트 게임&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;풀이&lt;/h2&gt;
&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons64061&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/64061&quot;&gt;크레인 인형뽑기 게임&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(board, moves):
    answer = 0
    n = len(board)
    stack = []
    for m in moves:
        for i in range(n):
            if board[i][m-1] != 0:
                stack.append(board[i][m-1])
                board[i][m-1] = 0
                # 인형삭제
                if len(stack) &amp;gt;= 2 and stack[-1] == stack[-2]:
                    stack.pop()
                    stack.pop()
                    answer += 2
                break
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons68644&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/68644&quot;&gt;두 개 뽑아서 더하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(numbers):
    answer = set()
    for i in range(len(numbers)):
        for j in range(i+1, len(numbers)):
            answer.add(numbers[i]+numbers[j])
    answer = sorted(answer)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;set 자료형으로 중복된 값이 삽입되지 않게 구현하였다. 시간복잡도는 $n^2 + n\log n \rightarrow O(n^2)$&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons42576&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42576&quot;&gt;완주하지 못한 선수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(participant, completion):
    d = {}
    for p in participant:
        if p in d:
            d[p] += 1
        else:
            d[p] = 1
    
    for c in completion:
        d[c] -= 1
    
    for k in d:
        if d[k] != 0:
            answer = k
            break
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;dictionary 자료형을 이용해서 돔영이인도 처리해주었음. 시간복잡도는 $3n \rightarrow O(n)$&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons72410&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/72410&quot;&gt;신규 아이디 추천&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;import string

def solution(new_id):
    # 1. 대문자-&amp;gt;소문자
    new_id = new_id.lower()
    # 2. 유효하지 않은 문자 제거
    valid_str = list(string.ascii_lowercase) + list(map(str, range(10))) + ['-', '_', '.']
    valid_str = set(valid_str)
    tmp = ''
    for i in range(len(new_id)):
        # O(1)
        if new_id[i] in valid_str:
            tmp += new_id[i]
    new_id = tmp
    # 3. 마침표 두 번 치환
    tmp = ''
    i = 0
    while i &amp;lt; len(new_id):
        if new_id[i] != '.':
            tmp += new_id[i]
            i += 1
            continue
        j = i
        while j &amp;lt; len(new_id) and new_id[j] == '.':
            j += 1
        i = j
        tmp += '.'
    new_id = tmp
    # 4. 처음이나 끝 '.' 제거
    if len(new_id) &amp;gt; 0 and new_id[0] == '.':
        new_id = new_id[1:]
    if len(new_id) &amp;gt; 0 and new_id[-1] == '.':
        new_id = new_id[:-1]
    # 5. 빈 문자열이라면 'a' 대입
    if new_id == '':
        new_id = 'a'
    # 6. 길이가 16자 이상이라면 15개까지만
    if len(new_id) &amp;gt;= 16:
        new_id = new_id[:15]
        # 마지막 '.' 처리
        if new_id[-1] == '.':
            new_id = new_id[:-1]
    # 길이가 2자 이하라면
    if len(new_id) &amp;lt;= 2:
        ch = new_id[-1]
        while len(new_id) &amp;lt; 3:
            new_id += ch
    return new_id
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;3번에서 마침표가 여러번 나오는 걸 하나의 마침표로 치환해주는게 조금 복잡해서 애먹은 문제..&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons42840&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42840&quot;&gt;모의고사&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(answers):
    answer = []
    st1 = [1, 2, 3, 4, 5]
    st2 = [2, 1, 2, 3, 2, 4, 2, 5]
    st3 = [3, 3, 1, 1, 2, 2, 4, 4, 5, 5]
    cnt1 = get_cnt(st1, answers)
    cnt2 = get_cnt(st2, answers)
    cnt3 = get_cnt(st3, answers)
    cnts = [cnt1, cnt2, cnt3]
    max_val = max(cnts)
    for i in range(3):
        if cnts[i] == max_val:
            answer.append(i+1)
    return answer

def get_cnt(st, answers):
    cnt = 0
    for i in range(len(answers)):
        if st[i % len(st)] == answers[i]:
            cnt += 1
    return cnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;k-httpsprogrammerscokrlearncourses30lessons42748&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42748&quot;&gt;K번째 수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(array, commands):
    answer = []
    for c in commands:
        i, j, k = c
        tmp = array[i-1:j]
        tmp.sort()
        answer.append(tmp[k-1])
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons42862&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42862&quot;&gt;체육복&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n, lost, reserve):
    students = [1] * (n+1)
    # 0번째 학생은 존재하지 않음
    students[0] = 0
    for l in lost:
        students[l] -= 1
    for r in reserve:
        students[r] += 1
    for r in reserve:
        # 여벌을 가지고 왔지만 도난당한 경우
        if students[r] == 1:
            continue
        if 1 &amp;lt;= r-1 and students[r-1] == 0:
            students[r-1] += 1
            students[r] -= 1
        elif r+1 &amp;lt;= n and students[r+1] == 0:
            students[r+1] += 1
            students[r] -= 1
    # 여벌을 가지고 왔지만 빌려주지 못한 경우를 위해 min 함수 적용
    students = [min(1, st) for st in students]
    return sum(students)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;생각보다 신경써야 할 케이스가 많아서 푸는데 시간이 좀 걸렸다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12901&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12901&quot;&gt;2016년&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(a, b):
    days = ['SUN', 'MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT']
    months = [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]
    # 월, 일, 요일
    month = day = 1
    now = 5
    while month != a or day != b:
        now = (now + 1) % len(days)
        day += 1
        # 한 달이 지나면
        if day &amp;gt; months[month]:
            day = 1
            month += 1 
    answer = days[now]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;요일에 해당하는 배열과 각 달의 일수를 저장한 배열을 이용하면 어렵지 않은 문제였다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12903&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12903&quot;&gt;가운데 글자 가져오기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    answer = ''
    if len(s) % 2 == 1:
        answer = s[len(s)//2]
    else:
        answer = s[len(s)//2-1] + s[len(s)//2]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons68935&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/68935&quot;&gt;3진법 뒤집기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    triple = ''
    while n &amp;gt;= 1:
        triple += str(n % 3)
        n //= 3

    i = len(triple)-1
    d = 1
    answer = 0
    while i &amp;gt;= 0:
        answer += int(triple[i]) * d
        d *= 3
        i -= 1
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;triple&lt;/code&gt;을 만들 때 이미 뒤집어져있으므로 바로 10진법으로 바꿔주면 된다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12906&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12906&quot;&gt;같은 숫자는 싫어&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(arr):
    answer = []
    i = 0
    while i &amp;lt; len(arr):
        j = i
        while j &amp;lt; len(arr) and arr[j] == arr[i]:
            j += 1
        answer.append(arr[i])
        i = j
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;슬라이딩 윈도우를 이용하여 풀어주었다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12910&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12910&quot;&gt;나누어 떨어지는 숫자 배열&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(arr, divisor):
    answer = []
    for a in arr:
        if a % divisor == 0:
            answer.append(a)
    
    if not answer:
        answer = [-1]
    answer.sort()
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12912&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12912&quot;&gt;두 정수 사이의 합&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(a, b):
    a, b = min(a, b), max(a, b)
    answer = 0
    for i in range(a, b+1):
        answer += i
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12915&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12915&quot;&gt;문자열 내 마음대로 정렬하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(strings, n):
    answer = sorted(strings)
    answer = sorted(answer, key=lambda x: x[n])
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;쉬운 문제지만 인덱스 n으로 정렬하는 것이 사전순정렬보다 우선순위이기 때문에 &lt;strong&gt;직관과 반대로 먼저 사전순정렬을 해준 다음에 인덱스 n으로 정렬해야 한다.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;p-y-httpsprogrammerscokrlearncourses30lessons12916&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12916&quot;&gt;문자열 내 p와 y의 개수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    s = s.lower()
    p_num = y_num = 0
    for ch in s:
        if ch == 'p':
            p_num += 1
        elif ch == 'y':
            y_num += 1
    return p_num == y_num
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12917&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12917&quot;&gt;문자열 내림차순으로 배치하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    s = list(s)
    s.sort(reverse=True)
    answer = ''.join(s)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12918&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12918&quot;&gt;문자열 다루기 기본&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    if not (len(s) == 4 or len(s) == 6):
        return False
    for ch in s:
        if not ch.isdigit():
            return False
    return True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;파이썬 string의 내장 메소드인 &lt;code class=&quot;highlighter-rouge&quot;&gt;isdigit()&lt;/code&gt;을 활용하면 더 쉽게 풀 수 있는 문제였다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12919&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12919&quot;&gt;서울에서 김서방 찾기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(seoul):
    idx = seoul.
    answer = &quot;김서방은 &quot; + str(idx) + &quot;에 있다&quot;
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12921&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12921&quot;&gt;소수 찾기&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;(틀린 코드)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    answer = 0
    for i in range(2, n+1):
        if is_prime(i):
            answer += 1
    return answer

def is_prime(n):
    i = 2
    while i*i &amp;lt;= n:
        if n % i == 0:
            return False
        i += 1
    return True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이 문제는 일반적인 소수를 찾는 방법을 주어진 범위에 모두 적용하면 효율성 테스트에서 틀린다ㅠㅠ 따라서 보다 효율적인 알고리즘을 사용해야하는데, &lt;strong&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%97%90%EB%9D%BC%ED%86%A0%EC%8A%A4%ED%85%8C%EB%84%A4%EC%8A%A4%EC%9D%98_%EC%B2%B4#%EC%97%90%EB%9D%BC%ED%86%A0%EC%8A%A4%ED%85%8C%EB%84%A4%EC%8A%A4%EC%9D%98_%EC%B2%B4%EB%A5%BC_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%EC%96%B8%EC%96%B4%EB%A1%9C_%EA%B5%AC%ED%98%84&quot;&gt;에라토스테네스의 체&lt;/a&gt;&lt;/strong&gt;라는 알고리즘을 사용하면 된다.&lt;/p&gt;

&lt;p&gt;(맞는 코드)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    array = [True] * (n+1)
    # 0과 1은 소수가 아니다.
    array[:2] = [False] * 2
    i = 2
    while i*i &amp;lt;=  n:
        if array[i]:
            j = 2*i
            while j &amp;lt;= n:
                array[j] = False
                j += i
        i += 1
    answer = sum(array)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12922&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12922&quot;&gt;수박수박수박수박수박수?&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    answer = ''
    arr = ['수', '박']
    for i in range(n):
        answer += arr[i % 2]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12925&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12925&quot;&gt;문자열을 정수로 바꾸기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    return int(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;역시 코테는 파이썬이 짱인듯하다..&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons70128&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/70128&quot;&gt;내적&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(a, b):
    answer = 0
    for i in range(len(a)):
        answer += a[i] * b[i]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12926&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12926&quot;&gt;시저 암호&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s, n):
    answer = ''
    for ch in s:
        if ch == ' ':
            answer += ch
        # 소문자라면
        elif ord('a') &amp;lt;= ord(ch) &amp;lt;= ord('z'):
            tmp = ord(ch)+n
            if tmp &amp;gt; ord('z'):
                tmp -= 26
            answer += chr(tmp)
        # 대문자라면
        else:
            tmp = ord(ch)+n
            if tmp &amp;gt; ord('Z'):
                tmp -= 26
            answer += chr(tmp)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;‘z’ 같은 경우에는 1을 더했을 때 ‘a’가 나와야 하므로 ‘z’보다 크면 26을 빼줘야하는게 이 문제의 핵심이었다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12928&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12928&quot;&gt;약수의 합&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    answer = 0
    for i in range(1, n+1):
        if n % i == 0:
            answer += i
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12930&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12930&quot;&gt;이상한 문자 만들기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(s):
    answer = ''
    i = 0
    for ch in s:
        if ch == ' ':
            answer += ch
            i  = 0
            continue
        if i % 2 == 0:
            answer += ch.upper()
        else:
            answer += ch.lower()
        i += 1
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;공백을 기준으로 단어를 분리해서 짝/홀수를 세어줘야 하기 때문에 공백을 만나면 &lt;code class=&quot;highlighter-rouge&quot;&gt;i=0&lt;/code&gt;이 된다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12931&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12931&quot;&gt;자릿 수 더하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    answer = 0
    string = str(n)
    for ch in string:
        answer += int(ch)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12932&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12932&quot;&gt;자연수 뒤집어 배열로 만들기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    string = str(n)[::-1]
    answer = []
    for ch in string:
        answer.append(int(ch))
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12933&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12933&quot;&gt;정수 내림차순으로 배치하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    answer = 0
    lst = list(str(n))
    lst.sort(reverse=True)
    answer = int(''.join(lst))
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12934&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12934&quot;&gt;정수 제곱근 판별&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n):
    i = 1
    x = -1
    while i*i &amp;lt;= n:
        if i*i == n:
            x = i
        i += 1
        
    if x == -1:
        answer = -1
    else:
        answer = (x+1) ** 2
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12935&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12935&quot;&gt;제일 작은 수 제거하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(arr):
    min_val = min(arr)
    answer = [x for x in arr if x != min_val]
    if not answer:
        answer.append(-1)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12937&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12937&quot;&gt;짝수와 홀수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(num):
    if num % 2 == 0:
        answer = 'Even'
    else:
        answer = 'Odd'
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons67256&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/67256&quot;&gt;키패드 누르기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(numbers, hand):
    answer = ''
    # 왼손, 오른손 위치
    l = (3, 0)
    r = (3, 2)
    for n in numbers:
        if n in (1, 4, 7):
            answer += 'L'
            l = n//3, 0
        elif n in (3, 6, 9):
            answer += 'R'
            r = n//3-1, 2
        else:
            c = (n//3, 1)
            # 0인 경우 예외 처리
            if n == 0:
                c = (3, 1)
            l_dist = distance(l, c)
            r_dist = distance(r, c)
            if l_dist &amp;lt; r_dist:
                l = c
                answer += 'L'
            elif r_dist &amp;lt; l_dist:
                r = c
                answer += 'R'
            else: # 두 거리가 같을때
                if hand == 'left':
                    answer += 'L'
                    l = c
                else:
                    answer += 'R'
                    r = c
    return answer

def distance(a, b):
    return abs(a[0]-b[0])+abs(a[1]-b[1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;numbers&lt;/code&gt;안에 있는 0을 따로 예외처리해줘야 한다. 그 부분만 빼면 평범한 구현문제인듯하다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12940&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12940&quot;&gt;최대공약수와 최소공배수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n, m):
    gcd = find_gcd(max(n, m), min(n, m))
    lcm = n * m / gcd
    answer = [gcd, lcm]
    return answer

def find_gcd(m, n):
    &quot;&quot;&quot;m has to be greater than or equal to n&quot;&quot;&quot;
    r = m % n
    while r &amp;gt; 0:
        m, n = n, r
        r = m % n
    return n
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12943&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12943&quot;&gt;콜라츠 추측&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(num):
    answer = 0
    while num != 1:
        answer += 1
        if num % 2 == 0:
            num //= 2
        else:
            num = num*3 + 1
        if answer &amp;gt;= 500:
            answer = -1
            break
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12944&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12944&quot;&gt;평균 구하기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(arr):
    cnt = len(arr)
    total = sum(arr)
    return total / cnt
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12947&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12947&quot;&gt;하샤드 수&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(x):
    d = 0
    x_str = str(x)
    for ch in x_str:
        d += int(ch)
    return x % d == 0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12948&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12948&quot;&gt;핸드폰 번호 가리기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(phone_number):
    answer = '*' * (len(phone_number)-4) + phone_number[-4:]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12950&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12950&quot;&gt;행렬의 덧셈&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(arr1, arr2):
    answer = [[0] * len(arr1[0]) for _ in range(len(arr1))]
    for i in range(len(answer)):
        for j in range(len(answer[0])):
            answer[i][j] += (arr1[i][j] + arr2[i][j])
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;x---n-httpsprogrammerscokrlearncourses30lessons12954&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12954&quot;&gt;x만큼 간격이 있는 n개의 숫자&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(x, n):
    answer = [x*i for i in range(1, n+1)]
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12969&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12969&quot;&gt;직사각형 별찍기&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;a, b = map(int, input().strip().split(' '))
for i in range(b):
    for j in range(a):
        print('*', end='')
    print()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons12982&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/12982&quot;&gt;예산&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(d, budget):
    answer = 0
    d.sort(reverse=True)
    while d:
        budget -= d.pop()
        if budget == 0:
            answer += 1
            break
        elif budget &amp;lt; 0:
            break
        answer += 1
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons17681&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/17681&quot;&gt;[1차] 비밀지도&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(n, arr1, arr2):
    answer = []
    for a, b, in zip(arr1, arr2):
        str1, str2 = bin(a)[2:], bin(b)[2:]
        # 공백처리
        if len(str1) &amp;lt; n:
            str1 = (n-len(str1)) * '0' + str1
        if len(str2) &amp;lt; n:
            str2 = (n-len(str2)) * '0' + str2
            
        tmp = ['#' if ch1 == '1' or ch2 == '1' else ' ' for 
              ch1, ch2 in zip(str1, str2)]
        answer.append(''.join(tmp))
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;파이썬 내장함수인 &lt;code class=&quot;highlighter-rouge&quot;&gt;bin&lt;/code&gt;을 사용하면 십진수를 이진법으로 쉽게 바꿔줄 수 있다. 또한 &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt;에 맞게 패딩처리도 해줘야한다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons42889&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/42889&quot;&gt;실패율&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(N, stages):
    d = {i: 0 for i in range(1, N+1)}
    total = len(stages)
    cnt = 1
    stages.sort()
    prev = stages[0]
    for i in range(1, len(stages)):
        # 범위를 이미 넘어선 경우
        if prev &amp;gt; N:
            break
            
        # 딕셔너리 업데이트
        if stages[i] != prev:
            d[prev] = cnt / total
            total -= cnt
            cnt = 1
            prev = stages[i]
        # 마지막 원소까지 d에 있는 경우
        elif i == len(stages)-1 and prev in d:
            cnt += 1
            d[prev] = total / cnt
        else:
            cnt += 1
            
    # stable sort: reverse=True말고 -를 붙여주어야 한다.
    answer = sorted(d.keys(), key=lambda x: -d[x])
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stages&lt;/code&gt;를 정렬해주고 직전의 값과 비교해주면서 딕셔너리를 업데이트 해주었다. 이때 &lt;code class=&quot;highlighter-rouge&quot;&gt;stages&lt;/code&gt;에 &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;보다 큰 값이 있을 수도 있으니 예외처리를 해주고, &lt;code class=&quot;highlighter-rouge&quot;&gt;[4, 4, 4, 4]&lt;/code&gt;같은 경우 마지막까지 범위안에 들어가므로 &lt;code class=&quot;highlighter-rouge&quot;&gt;elif&lt;/code&gt;조건을 추가했다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;em&gt;stable sort&lt;/em&gt;를 위해 파이썬의 내장함수인 &lt;code class=&quot;highlighter-rouge&quot;&gt;sorted&lt;/code&gt;를 사용했는데 &lt;code class=&quot;highlighter-rouge&quot;&gt;reverse=True&lt;/code&gt;를 옵션으로 줘버리면 &lt;em&gt;reverse stable sort&lt;/em&gt;가 되기 때문에 &lt;code class=&quot;highlighter-rouge&quot;&gt;key&lt;/code&gt; 파라미터에 마이너스(-)를 적용한 람다함수를 값으로 주었다.&lt;/p&gt;

&lt;h3 id=&quot;httpsprogrammerscokrlearncourses30lessons17682&quot;&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/30/lessons/17682&quot;&gt;[1차] 다트 게임&lt;/a&gt;&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;def solution(dartResult):
    answer = 0
    # 점수들을 담은 스택
    stack = []
    for i, ch in enumerate(dartResult):
        if ch.isdigit():
            # 숫자가 이어지는 경우
            if stack and dartResult[i-1].isdigit():
                stack.append(stack.pop()*10 + int(ch))
            else:
                stack.append(int(ch))
        elif ch in ('S', 'D', 'T'):
            if ch == 'S':
                m = 1
            elif ch == 'D':
                m = 2
            elif ch == 'T':
                m = 3
            # 점수를 바꿔준다.
            stack[-1]  = stack[-1] ** m
        else:
            # 스타상 처리
            if ch == '*':
                if len(stack) &amp;gt;= 2:
                    stack[-1] *= 2
                    stack[-2] *= 2
                else:
                    stack[-1] *= 2
            else: # 아차상 처리
                stack[-1] *= -1
        
    answer = sum(stack)
    return answer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;지금까지 획득한 점수들을 담은 배열을 스택으로 생각하고 처리해주면 되는 문제였다.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jan 2021 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/problem-solving/2021/01/25/Programmers-Lv1-1.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/problem-solving/2021/01/25/Programmers-Lv1-1.html</guid>
        
        <category>python</category>
        
        <category>coding-test</category>
        
        <category>programmers</category>
        
        
        <category>Problem-Solving</category>
        
      </item>
    
      <item>
        <title>Django 기본 명령어 모음</title>
        <description>&lt;pre&gt;&lt;code class=&quot;language-python3&quot;&gt;# run server
&amp;gt; python manage.py runserver {port_number}

# app 만들기
&amp;gt; python manage.py startapp {app_name}

# migration
&amp;gt; python manage.py makemigrations {app_name}
&amp;gt; python manage.py migrate {app_name}

# check
&amp;gt; python manage.py sqlmigrate {app_name} 0001_initial
&amp;gt; python manage.py dbshell
&amp;gt; python manage.py showmigrations {app_name}
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Mon, 25 Jan 2021 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/django/2021/01/25/Django-commands.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/django/2021/01/25/Django-commands.html</guid>
        
        <category>python</category>
        
        <category>django</category>
        
        <category>backend</category>
        
        <category>server</category>
        
        
        <category>django</category>
        
      </item>
    
      <item>
        <title>Pix2Pix, CycleGAN</title>
        <description>&lt;p&gt;이번 포스팅에서는 image-to-image translation model의 조상님격인 Pix2Pix와 CycleGAN에 대해서 알아보겠습니다. Pix2Pix를 먼저 다루고 그 후에 CycleGAN에 대해 다루려고 합니다.&lt;/p&gt;

&lt;p&gt;아래는 Pix2Pix 원논문에 나와있는 image-to-image translation을 잘 보여주는 사진입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/img2img.png&quot; alt=&quot;image-to-image translation&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;pix2pix&quot;&gt;Pix2Pix&lt;/h1&gt;

&lt;p&gt;vanilla GAN은 random noise vector $z$를 output image $y$로 보내는 mapping function $G: x \rightarrow y$를 학습합니다. 그러나 conditional GAN은 observed image $x$와 random noise vector $z$를 $y$로 보내는 mapping을 학습합니다. 즉, $G: { x, z } \rightarrow y$를 학습합니다. 이를 그림으로 나타내면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/training-pix2pix.png&quot; alt=&quot;Pix2Pix&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;objective&quot;&gt;Objective&lt;/h2&gt;

&lt;p&gt;conditional GAN의 objective function은 아래와 같이 표현될 수 있습니다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L_\textbf{cGAN}}(G, D) = \underset{x, y}{\mathbb{E}}[\log D(x, y)] +\underset{x, z}{\mathbb{E}}[\log (1-D(x, G(x, z)))]$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;즉, 기존 GAN과 다르게 D는 $x, y$가 진짜 이미지에서 온 pair인지 판별하게 됩니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;또한 GAN objective에 L1 loss를 추가해줌으로써 G로 하여금 D를 속일뿐만 아니라 output과 ground truth가 비슷하게 만드는 태스크를 추가합니다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L_\text{L1}}(G) = \mathbb{E}_{x,y,z}[\vert\vert y-G(x,z) \underset{1}{\vert\vert}]$&lt;/p&gt;

&lt;p&gt;따라서 final objective는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;$G^{*} = \arg \underset{G}{\min} \underset{D}{\max}\mathcal{L_\textbf{cGAN}}(G, D)+\lambda\mathcal{L_\text{L1}}(G)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;원래는 $z$에다가 Gaussian noise를 넣어주는 것이 정석인데, 여기서는 &lt;em&gt;dropout&lt;/em&gt;을 적용하는 것으로 대체합니다. train/test시에 모두 &lt;em&gt;dropout&lt;/em&gt;의 형태로 noise를 넣어주는 것입니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/pix2pix-losses.png&quot; alt=&quot;Pix2Pix Losses&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 두 loss term을 모두 합쳐준게 제일 결과가 좋습니다.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;

&lt;p&gt;구체적인 네트워크 구조에 대해서 설명하겠습니다. image-to-image translation model에서는 input과 output이 우리가 눈으로 보기에는 다른 구조를 가지지만 사실은 같은 underlying structure에서 rendering을 한 것이라고 주장합니다. &lt;strong&gt;따라서, input의 구조와 output의 구조가 align 되어야합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이 논문은 U-Net shape을 가진 G에 skip connection을 추가해줌으로써 encoder-decoder network의 information bottleneck 현상을 방지합니다. low-level information도 잘 전달되도록 하는 것이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;L1 Loss를 우리의 비용함수에 포함하는 것은 G가 low frequencies를 제대로 capture하도록 강제하는 것으로 볼 수 있습니다. 이로인해 D는 high-frequency structure만 잡아내면 되고, 이 논문의 저자들은 &lt;em&gt;PatchGAN&lt;/em&gt;이라는 새로운 구조를 도입합니다.&lt;/p&gt;

&lt;p&gt;D는 G가 생성한 전체 이미지를 보는 대신에, $N \times N$ 크기의 patch를 보고 real/fake 여부를 판별합니다. 그 후에 이것들을 평균내 D의 output으로 넣어줍니다. 이렇게 함으로써 이미지의 high-frequency를 모델링했다고 합니다. 경험적으로 $70 \times 70$짜리 패치정도가 적당했다고 하네요.&lt;/p&gt;

&lt;p&gt;또한 특이한 점은 test시에도 train data의 통계량을 사용하지 않고 test data의 통계량을 사용했다고 합니다. 개인적으로는 $z$를 dropout으로 넣었기때문에 이 방법이 더 효과적이었던 것 같습니다.&lt;/p&gt;

&lt;p&gt;그 뒤에는 여러 실험결과가 나오는데, 저는 이 부분은 생략하고 CycleGAN으로 넘어가겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;cyclegan&quot;&gt;CycleGAN&lt;/h1&gt;

&lt;p&gt;먼저, Cycle GAN으로 생성된 결과부터 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/cycle-img.png&quot; alt=&quot;cycle-gan&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금부터 설명드릴 Cycle GAN은 Pix2Pix의 문제점인 data insuffiency를 해결한 논문입니다. Pix2Pix는 paired image dataset이 필요합니다. 그러나 실제로 그런 데이터는 흔하지 않습니다.&lt;/p&gt;

&lt;p&gt;Cycle GAN은 &lt;strong&gt;서로 다른 도메인&lt;/strong&gt;의 이미지들간 mapping을 구해주는 모델입니다. 즉, $G: X \rightarrow Y$라는 translator와 $F: Y \rightarrow X$라는 translator를 구해줍니다. $G^{-1}= F$의 관계가 성립합니다.&lt;/p&gt;

&lt;p&gt;하지만 우리가 neural network에게 $G^{-1}= F$가 성립하도록 explicit하게 제재를 가해주는 방법이 없습니다. 따라서 이 논문들의 저자는 &lt;strong&gt;Cycle Consistency Loss&lt;/strong&gt;를 더함으로써 $F(G(x)) \approxeq x$, $G(F(x)) \approxeq y$가 성립하도록 해주었습니다.&lt;/p&gt;

&lt;p&gt;전체적인 네트워크 구조는 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/cycle-arc.png&quot; alt=&quot;cycle-gan&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;objective-1&quot;&gt;Objective&lt;/h2&gt;

&lt;h3 id=&quot;adversarial-loss&quot;&gt;Adversarial Loss&lt;/h3&gt;

&lt;p&gt;generated image가 target domain에서 왔는지 판별하는 D를 통해 adversarial 하게 학습합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/cycle-loss1.png&quot; alt=&quot;cycle-gan&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cycle-consistency-loss&quot;&gt;Cycle Consistency Loss&lt;/h3&gt;

&lt;p&gt;$G, F$가 역함수 관계에 있을 수 있도록 학습합니다. 이는 mode collapsing 문제도 해결해준다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/cycle-loss2.png&quot; alt=&quot;cycle-gan&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;full-objective&quot;&gt;Full Objective&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/cycle-loss3.png&quot; alt=&quot;cycle-gan&quot; /&gt;&lt;/p&gt;

&lt;p&gt;언급한 Loss를 모두 결합하면 위와 같습니다.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.07004.pdf&quot;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot;&gt;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2021 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2021/01/13/Pix2Pix.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2021/01/13/Pix2Pix.html</guid>
        
        <category>GAN</category>
        
        <category>Pix2Pix</category>
        
        <category>pix2pix</category>
        
        <category>CycleGAN</category>
        
        <category>deep-learning</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>InfoGAN</title>
        <description>&lt;p&gt;이번 포스팅에서는 InfoGAN에 대해서 알아보겠습니다. GAN에 대한 설명은 생략하겠습니다.&lt;/p&gt;

&lt;p&gt;먼저, InfoGAN의 motivation 부터 알아보려고 합니다. GAN을 통해 우리는 latent variable $z$에서 실제 데이터 $x$로 가는 mapping을 학습합니다. 그래서 $z$를 latent representation이라고 합니다. 그러나 아쉽게도 GAN은 entangled representation을 학습합니다. 아래는 disentagled/entagled의 차이점을 시각화한 그림입니다. (&lt;a href=&quot;https://science.sciencemag.org/content/295/5552/7/tab-figures-data&quot;&gt;이미지 출처&lt;/a&gt;) &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://science.sciencemag.org/content/sci/295/5552/7/F1.medium.gif&quot; alt=&quot;swiss roll&quot; /&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;D보다는 B에서 manifold가 깔끔하게 학습되었고, GAN을 통해서 이런 깔끔한 manifold, 즉 &lt;strong&gt;disentangled&lt;/strong&gt; representaion을 학습함으로써 이미지의 퀄리티와 explainability를 동시에 향상시켰다는 것이 이 논문의 주요 내용입니다.&lt;/p&gt;

&lt;h2 id=&quot;mutual-information-for-inducing-latent-codes&quot;&gt;Mutual Information for Inducing Latent Codes&lt;/h2&gt;

&lt;p&gt;원래 GAN에서는 input noise vector로 $z$를 넣어줍니다. 대부분 $z$는 $\mathcal{N}[\mathbf{0}, \sigma^{2} \mathbf{I}]$를 따른다고 가정합니다. generator는 어떤 제한없이 $z$를 그대로 받아서 이미지를 생성하게 됩니다. 결과적으로, $z$의 각 차원이 데이터의 semantic feature와 대응되지 않는다는 문제점이 생깁니다.&lt;/p&gt;

&lt;p&gt;하지만 실제로 이미지 데이터는 semantically meaningful factors로 분해될 수 있습니다. 예를 들어, MNIST에서는 숫자의 휘어진 정도 또는 numerical identity(0-9)가 있겠습니다. 이런 특징들을 $z$가 잡을 수 있다면 훨씬 해석이 쉬운 동시에 원하는 이미지를 생성하기도 쉬울 것입니다.&lt;/p&gt;

&lt;p&gt;이 논문에서는, input noise vector를 크게 두 파트로 분류합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$z$: incompressible한 noise&lt;/li&gt;
  &lt;li&gt;$c$: latent code - data의 semantic feature에 해당합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;여기서 $c$는 factored distribution을 가정합니다. 왜냐하면 이미지의 각 salient feature가 독립이라고 가정하는 것이 합리적이기 때문입니다. 수식으로 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$P(c_{1}, …, c_{L}) = \prod_{i=1}^{L}P(c_{i})$&lt;/p&gt;

&lt;p&gt;우리는 generator에 $z, c$를 인풋으로 넣어줍니다. 그러면 generator는 $G(z, c)$로 표현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이제 이 논문에 key idea를 설명하겠습니다. 이 논문은 &lt;strong&gt;latent code $c$와 $G(z, c)$간에 high mutual information이 있어야 한다는 제약조건을 상정합니다. 따라서, $I(c;G(z, c))$는 높아야 합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$I(X;Y)$란 $Y$가 관측되었을 때 $X$의 불확실성이 감소되는 정도를 의미합니다. 만약 $X$, $Y$가 indepentdent하다면 $I(X;Y)=0$이 성립합니다. 만약 information theory에 대해서 생소하다면 &lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_entropy&quot;&gt;링크&lt;/a&gt;를 참조하면 좋을 것 같습니다.&lt;/p&gt;

&lt;p&gt;따라서 우리의 목표는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$\min_{G}\max_{D}V_{I}(D, G) = V(D, G)-\lambda I(c;G(z,c))$&lt;/p&gt;

&lt;p&gt;여기서 regularization term에 해당하는 $\lambda I(c;G(z,c))$는 latent code $c$의 information이 generation process에서 소실되지 않도록 강제합니다.&lt;/p&gt;

&lt;h2 id=&quot;variational-mutual-information-maximization&quot;&gt;Variational Mutual Information Maximization&lt;/h2&gt;

&lt;p&gt;하지만 실제로 $I(c;G(z,c))$를 직접 최대화하는 것에는 어려움이 있습니다. 왜냐하면 $P(c \vert x)$를 알아야 하기 때문입니다. &lt;strong&gt;따라서 우리는 variational lower bound 테크닉을 이용해서 $P(c \vert x)$를 $Q(c \vert x)$로 근사합니다.&lt;/strong&gt; 이제 variational lower bound에 대해서 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;$I(c;G(z,c))=H(c)-H(c\vert G(z, c)=\mathbb{E_{x \thicksim G(z, c)}}[\mathbb{E_{c’ \thicksim P(c \vert x)}[log P(c’ \vert x)]}] + H(c)$&lt;/p&gt;

&lt;p&gt;$=\mathbb{E_{x \thicksim G(z, c)}}[D_{KL}(P(c’ \vert x) \vert\vert Q(c’ \vert x))] +\mathbb{E_{c’ \thicksim P(c \vert x)}}[log Q(c’ \vert x)] + H(c)$&lt;/p&gt;

&lt;p&gt;$\geq \mathbb{E_{x \thicksim G(z, c)}}[\mathbb{E_{c’ \thicksim P(c \vert x)}}[log Q(c’ \vert x)]] + H(c)$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;font color=&quot;red&quot;&gt;$\mathcal{L_{I}}(G, Q) = \mathbb{E_{x \thicksim G(z, c)}}[\mathbb{E_{c' \thicksim P(c \vert x)}}[log Q(c' \vert x)]] + H(c)$&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서도 문제가 발생합니다. $\mathbb{E_{c’ \thicksim P(c \vert x)}}[log Q(c’ \vert x)]$에 $P(c \vert x)$가 들어간다는 점입니다. 하지만 수리통계학의 &lt;em&gt;lemma&lt;/em&gt;에 의해 해결가능합니다. (&lt;a href=&quot;http://aoliver.org/correct-proof-of-infogan-lemma&quot;&gt;링크&lt;/a&gt;에 자세히 나와있습니다.) 구체적인 수식을 살펴보겠습니다. $G(z, c)$는 $P_{G}(x \vert z, c)$로 새로 나타낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;$\mathbb{E_{x \thicksim G(z, c)}}[\mathbb{E_{c’ \thicksim P(c \vert x)}}[log Q(c’ \vert x)]] = \mathbb{E_{x \thicksim P_{G}(x \vert z, c), c’ \thicksim P(c’ \vert x)}}[logQ(c’ \vert x)]$&lt;/p&gt;

&lt;p&gt;$=\mathbb{E_{x \thicksim P_{G}(x \vert z, c), c \thicksim x, c’ \thicksim P(c’ \vert x)}}[logQ(c’ \vert x)]=\mathbb{E_{x \thicksim G(z, c), c \thicksim p(c)}}[logQ(c’ \vert x)]$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;font color=&quot;red&quot;&gt;${L_{I}}(G, Q) = \mathbb{E_{x \thicksim G(z, c), c \thicksim p(c)}}[logQ(c' \vert x)] + H(c)$&lt;/font&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그러면 이제 마지막식은 Monte Carlo simulation으로 손쉽게 구할 수 있고 최종목표식은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$\min_{G}\max_{D}V_{I}(D, G) = V(D, G)-\lambda{L_{I}}(G, Q)$&lt;/p&gt;

&lt;p&gt;수식으로만 보니 와닿지 않기 때문에 이 논문을 요약하여 한장의 그림으로 나타내어 보았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/infoGAN.png&quot; alt=&quot;InfoGAN structrure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;핵심은 generator로 하여금 semantic feature/latent code c를 복원할 수 있을 만큼 생성을 해라!가 되겠습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;실제로 코드로 구현할 때는 $Q$를 neural network로 구현하되 $D$와 모든 convolution layer를 공유하고 $Q(c \vert x)$를 위한 FC layer만 추가해주었다고 합니다. 이로써 $Q$에 대한 cost는 vanilla GAN과 거의 동일해집니다.&lt;/p&gt;

&lt;p&gt;$c_{i}$가 categorical 변수일 때는 $Q(c _{i}\vert x)$를 구하는데 softmax nonlinearity를 이용했고 continuous 변수일 때는 factored Gaussian으로 충분했다고 합니다.&lt;/p&gt;

&lt;p&gt;또한 $\lambda$에 대한 튜닝도 쉬웠다고 하는데요. $c_{i}$가 categorical 변수일 때는 $\lambda$를 1로 두고, continuous 변수일 때는 그것보다 작은 값을 이용했다고 언급하고 있습니다. 마지막으로 GAN structure는 DC-GAN을 이용했습니다.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;mutual-information-maximization&quot;&gt;Mutual Information Maximization&lt;/h3&gt;

&lt;p&gt;InfoGAN의 학습이 잘 되었다면 lower bound가 $H(c)$로 maximize 됩니다. 이말인즉슨 bound가 tight한 동시에 maximal mutual information을 얻었다는 뜻입니다. 원논문의 Fig1이 이를 시각화해서 보여주고 있습니다. 화질이 좋지 않아서 첨부하진 않았습니다. vanilla GAN은 lower bound가 $H(c)$로 maximize되지 않지만 InfoGAN은 빠르게 수렴하고 있습니다. 이는 vanilla GAN에서 generator는 이미지의 semantic feature를 잡아내지 못함을 뜻합니다.&lt;/p&gt;

&lt;h2 id=&quot;disentangled-representation&quot;&gt;Disentangled Representation&lt;/h2&gt;

&lt;p&gt;MNIST에 대해 살펴보겠습니다. $c_{1} \thicksim Cat(K=10, p=0.1)$을 사용했고 $c_{2}, c_{3}$은 continous variations를 나타내기 위한 변수로 사용했다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/infogan-mnist.png&quot; alt=&quot;InfoGAN MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리의 예상대로 $c_{1}$은 digit type을 나타내는 피쳐에 해당합니다. 그리고 $c_{2}, c_{3}$는 각각 rotation, width에 해당하는 피쳐가 되겠습니다.&lt;/p&gt;

&lt;p&gt;다른 데이터셋을 시각화한 결과가 궁금하시다면 원논문을 참조하시면 됩니다. latent code $c$가 이미지의 semantic feature를 잘 잡아냄을 알 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.03657.pdf&quot;&gt;InfoGAN: Interpretable Representation Learning by
Information Maximizing Generative Adversarial Nets&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jan 2021 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2021/01/07/InfoGAN.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2021/01/07/InfoGAN.html</guid>
        
        <category>GAN</category>
        
        <category>InfoGAN</category>
        
        <category>deep-learning</category>
        
        <category>representation-learning</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>Wasserstein GAN</title>
        <description>&lt;p&gt;오늘 포스팅은 Wasserstein GAN에 대해 알아보겠습니다. 
모두 아시다시피, GAN은 data의 distribution을 배우는 generative model에 해당합니다. 그렇다면 probability distribution, 즉 확률분포를 학습한다는 것은 정확히 무슨 뜻일까요? WGAN에 대해 알아보기 전에 보다 일반적인 얘기를 먼저 하고 넘어가려고 합니다.&lt;/p&gt;

&lt;p&gt;parametric한 approach에서는 우리 데이터의 분포가 $P_{\theta} \text{ where }\theta \in \mathbb{R^{d}}$에 속한다고 가정하고 data의 likelihood를 최대화하는 방향으로 $\theta$를 추정합니다.&lt;/p&gt;

&lt;p&gt;$\hat{\theta} = argmax_{\theta}\frac{1}{m}\sum_{i=1}^{m}\log P_{\theta}(x^{(i)})$&lt;/p&gt;

&lt;p&gt;여기서 log는 단조 증가 함수이므로 위의 식이 성립합니다. asymptotic 하게 위의 식은 $KL(\mathbb{P_{r}} \vert\vert \mathbb{P_{\theta}})$를 최소화하는 것과 같습니다.&lt;/p&gt;

&lt;p&gt;확률분포간의 거리를 재는 방법이 있다면 우리는 데이터를 통해 추정한 분포와 실제 분포와의 거리를 최소화하려고 할 것이고, KL divergence는 일반적으로 분포간의 거리/유사도를 측정하는데 가장 많이 쓰이는 metric이기 때문에 우리의 직관과 매우 일치합니다.&lt;/p&gt;

&lt;h2 id=&quot;different-distances&quot;&gt;Different Distances&lt;/h2&gt;

&lt;p&gt;그러나 확률분포간의 거리를 재는 방법이 KL divergence 하나만 있는 것은 아닙니다. 이 논문에서는 4가지 방법들에 대해 간단하게 소개를 해줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Total Variation&lt;/em&gt; distance: $\delta(\mathbb{P_{r}}, \mathbb{P_{g}}) = \sup_{A \in \sum} \vert \mathbb{P_{r}}(A)-\mathbb{P_{g}}(A) \vert$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Kullback-Leibler&lt;/em&gt; divergence: $KL(\mathbb{P_{r}} \vert\vert \mathbb{P_{g}}) = \int log\frac{P_{r}(x)}{P_{g}(x)}P_{r}(x)d\mu(x)$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Jensen-Shannon&lt;/em&gt; divergence: $JS(\mathbb{P_{r}} , \mathbb{P_{g}}) = KL(\mathbb{P_{r}} \vert\vert \mathbb{P_{m}})+KL(\mathbb{P_{g}} \vert\vert \mathbb{P_{m}})$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Earth-Mover&lt;/em&gt; distance: $W(\mathbb{P_{r}} , \mathbb{P_{g}}) = \inf_{\gamma \in \prod{(\mathbb{P_{r}} , \mathbb{P_{g}})}} \mathbb{E_{(x, y)\thicksim\gamma}}[\vert\vert x-y \vert\vert]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;논문에서 제시한 example을 한 번 살펴봅시다. $Z \thicksim U[0, 1]$인 확률변수라고 하고 $\mathbb{P_{0}}$을 $(0, Z)$의 분포, $\mathbb{P_{\theta}}$를 $(\theta, Z)$의 분포라고 합니다. 앞에서 소개한 여러 distance metric에 따르면 두 분포의 distance는 다음과 같습니다. 유도과정은 생략하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$W(\mathbb{P_{0}} , \mathbb{P_{\theta}}) = \vert \theta \vert$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$JS(\mathbb{P_{0}} , \mathbb{P_{\theta}}) = \log2 \text{ if } \theta \neq 0 \text{ else } 0$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$KL(\mathbb{P_{0}} \vert\vert \mathbb{P_{\theta}}) = +\infty \text{ if } \theta \neq 0 \text{ else } 0$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\delta(\mathbb{P_{0}}, \mathbb{P_{\theta}}) = 1 \text{ if } \theta \neq 0 \text{ else } 0$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;이때, 여기서 흥미로운 점은 $\theta \in [-\infty, +\infty]$에서 연속인 함수는 EM distance 밖에 없다는 사실입니다.&lt;/strong&gt; 그림으로 나타내면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-dis.png&quot; alt=&quot;new loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼편은 EM distance를 나타낸 것이고 오른편은 JS distance를 나타낸 그림입니다. 그러나 original GAN paper에서는 JS divergence를 기준으로 generator를 업데이트 해줍니다. 만약 그림과 같은 상황이 펼쳐진다면, gradient는 0이 되고 정상적인 학습이 이루어지지 않을 것입니다. 그에 비해 EM distance를 기준으로 generator를 학습한다면 원활한 학습이 이루어질 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;결국, 이 논문의 essence는 NN으로 하여금 더 학습이 용이한 확률분포 distance metric을 정의하고 이를 기반으로 GAN을 학습시켰더니 더 잘 나왔다는 것입니다.&lt;/strong&gt; 여기서 논문에 나와있는 Theorem 1,2를 보고 가겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-thrm1.png&quot; alt=&quot;Theorem 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1번은 우리가 estimate 하고자하는 parameter의 space가 continuous 하다면 $W(\mathbb{P_{r}}, \mathbb{P_{\theta}})$도 continuous 하다는 내용입니다. 2번은 mapping function $g$가 &lt;em&gt;locally Lipschitz&lt;/em&gt;하고 regularity assumption 1을 만족한다면 $W(\mathbb{P_{r}}, \mathbb{P_{\theta}})$가 거의 모든 곳에서 미분가능하고, 이는 역전파를 가능하게 합니다. 3번에서는 1, 2가 $JS(\mathbb{P_{r}}, \mathbb{P_{\theta}}), KL(\mathbb{P_{r}}\vert\vert \mathbb{P_{\theta}})$에 대해서는 성립하지 않는다는 내용입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;여기서 핵심은 $W(\mathbb{P_{r}}, \mathbb{P_{\theta}})$가 거의 모든 곳에서 미분가능하기 때문에 올바른 gradient를 전달한다는 것입니다.&lt;/strong&gt; 그에 비해 $JS(\mathbb{P_{r}}, \mathbb{P_{\theta}}), KL(\mathbb{P_{r}}\vert\vert \mathbb{P_{\theta}})$는 그렇지 않아서 학습에 차질이 생깁니다. &lt;em&gt;locally Lipschitz&lt;/em&gt; 하다는 것은 rough하게 말하자면 parameter space가 compact하고, 일정한 거리안에 모여있다는 뜻입니다. 이에 대해서는 뒤에서도 나옵니다. regularity asssumption은 확률분포에 대한 조건인데, 매우 loose한 조건이라 이를 만족하지 않는 확률분포를 찾아보기가 더 힘듭니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;따라서 우리는 EM distance를 기준으로 GAN을 학습시키면 되겠습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-thrm2.png&quot; alt=&quot;Theorem 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1-4까지 읽어보시면, $W(\mathbb{P_{r}}, \mathbb{P_{\theta}}) \rightarrow 0$와 generator가 mapping 하는 분포가 데이터의 분포에 수렴하는 것이 동치임을 알 수 있습니다. 또한 KL, JS divergence 보다 EM distance가 weak한 개념이기 때문에 최적화가 보다 용이할 것이라고 기대할 수 있습니다. 논문에서는 이를 EM distance가 매우 sensible하다는 근거로 언급합니다.&lt;/p&gt;

&lt;h2 id=&quot;wasserstein-gan&quot;&gt;Wasserstein GAN&lt;/h2&gt;

&lt;p&gt;그러나 $W(\mathbb{P_{r}} , \mathbb{P_{g}}) = \inf_{\gamma \in \prod{(\mathbb{P_{r}} , \mathbb{P_{g}})}} \mathbb{E_{(x, y)\thicksim\gamma}}[\vert\vert x-y \vert\vert]$ 이 식을 직접적으로 다루기엔 문제가 있습니다. infimum(그냥 minimum으로 생각하시면 편합니다.) term이 intractible 하기 때문입니다. 그래서 우리는 Kantorovich-Rubinstein duality를 활용해서 다음과 같이 바꿔줍니다.&lt;/p&gt;

&lt;p&gt;$W(\mathbb{P_{r}} , \mathbb{P_{g}}) = \sup_{\vert\vert f\vert\vert_{L \leq 1} } \mathbb{E_{x \thicksim P_{r}}}[f(x)] - \mathbb{E_{x \thicksim P_{\theta}}}[f(x)]$&lt;/p&gt;

&lt;p&gt;이때, $\vert\vert f \vert\vert_{L \leq 1}$는 &lt;em&gt;locally Lipschitz&lt;/em&gt;를 만족시키기 위한 조건입니다. 그러나 실제로는 이 조건이 NN의 flexibility를 제한하거나 gradient가 saturate 되는 등 여러 문제가 생기기 때문에 $\vert\vert f \vert\vert_{L \leq K}$를 사용했다고 합니다. 이는 gradient clipping을 통해서 쉽게 구현할 수 있습니다. 또한 식을 보면 우리가 기존에 학습시키는 NN과 크게 다르지 않은, tractible한 식이 됩니다.&lt;/p&gt;

&lt;p&gt;어째됐든, 이제 우리의 문제는 $\max_{w \in \mathcal{W}} \mathbb{E_{x \thicksim P_{r}}}[f(x)] - \mathbb{E_{x \thicksim P_{\theta}}}[f(x)]$로 바뀌게 됩니다. $\mathcal{W}$는 앞서 말씀드린 $\vert\vert f \vert\vert_{L \leq K}$를 만족하는 공간입니다. 요약하면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-thrm3.png&quot; alt=&quot;Theorem 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 이제 기존 GAN과 마찬가지로 $W(\mathbb{P_{r}} , \mathbb{P_{g}})$에 대해 generator는 minimize, discriminator는 maximize를 하는 minimax problem을 풀면 됩니다. 수도코드는 매우 간단합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-pseudo.png&quot; alt=&quot;Pseudo Code&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 설명드릴 점은 두 가지 입니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;weight clipping을 통해 Lipschitz constraint를 구현했으나, 이건 좋은 방법이 아닙니다. weight의 norm을 제한한다면 gradient가 stuck 되는 현상이 나타날 수 있습니다. 또한 모멘텀을 사용하는 Adam optimizer를 사용했을때 오히려 역효과가 나기 때문에 비교적 학습이 오래 걸리는 RMSProp optimizer를 사용했습니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;기존 GAN은 generator가 mapping을 제대로 못하는 상황에서 discriminator가 optimal 상태에 도달하기가 쉽고, 이는 gradient가 죽어버리는 결과를 가져옵니다. 그러나 WGAN에서는 discriminator가 optimal 상태에 도달하는 것이 generator에게도 도움이 됩니다.&lt;/strong&gt; 이로 인해 mode collapse도 방지할 수 있다고 논문에서는 주장합니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;기존 GAN과 WGAN의 그라디언트를 나타내는 그림을 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-grads.png&quot; alt=&quot;Gradients&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존 GAN과 다르게 WGAN에서는 그라디언트가 linear 하기 때문에 saturate 되지 않고, 의미있는 정보를 전달할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;empirical-results-meaningful-loss-metric&quot;&gt;Empirical Results: Meaningful loss metric&lt;/h2&gt;

&lt;p&gt;마지막으로 설명드릴 내용은 &lt;strong&gt;EM distance가 meaningful loss metric&lt;/strong&gt;으로 사용될 수 있다는 점입니다. 그림으로 한 번 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/wgan-loss.png&quot; alt=&quot;EM loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/js-loss.png&quot; alt=&quot;JS loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;JS divergence 보다 EM distance가 우리 직관에 더 일치하는 결과를 보여주고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;이번 포스팅에서는 GAN의 loss metric을 개선한 Wasserstein GAN에 대해 알아보았습니다. 수학적인 내용이 많아 쉽지는 않지만 아이디어 자체는 심플한 논문입니다. 이를 더 개선한 &lt;a href=&quot;https://arxiv.org/pdf/1704.00028.pdf&quot;&gt;Improved Training of Wasserstein GANs&lt;/a&gt; 논문이 있는데, 다음에 기회가 되면 다뤄보려고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1701.07875.pdf&quot;&gt;Wasserstein GAN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2020/12/29/Wasserstein-GAN.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2020/12/29/Wasserstein-GAN.html</guid>
        
        <category>GAN</category>
        
        <category>WGAN</category>
        
        <category>Wasserstein-GAN</category>
        
        <category>deep-learning</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>Improved Techniques for Training GANs</title>
        <description>&lt;p&gt;오늘 리뷰할 논문은 Improved Techniques for Training GANs로, 이 논문은 아래와 같이 크게 세 가지 내용으로 이루어져 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GAN의 convergence를 위한 휴리스틱한 방법론들&lt;/li&gt;
  &lt;li&gt;사람이 측정한 것과 비슷하게 이미지 퀄리티를 측정하는 방법&lt;/li&gt;
  &lt;li&gt;Semi-supervised learning에 GAN을 이용하는 법과 그 중요성&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;편의상 generator를 G, discriminator를 D라고 칭하겠다.&lt;/p&gt;

&lt;h2 id=&quot;toward-convergent-gan-training&quot;&gt;Toward Convergent GAN Training&lt;/h2&gt;

&lt;h3 id=&quot;feature-matching&quot;&gt;Feature Matching&lt;/h3&gt;

&lt;p&gt;D의 direct output에 맞춰 업데이트되는 G와 다르게 이 논문에서는 새로운 objective를 설정했다. $\mathbf{f(x)}$를 D의 intermediate layer output이라고 했을 때, G의 새로운 목표는 $\vert\vert \mathbb{E_{x \thicksim p_{data}}}\mathbf{f(x)} - \mathbb{E_{z \thicksim p_{z}}}\mathbf{f}(G(z))\vert\vert^{2}$를 최소화하는 것이 된다.(D는 기존의 방식대로 학습됨) &lt;strong&gt;즉, 직접적인 비용이 아니라 D가 기대하는 feature에 G를 맞추는 것이다.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;minibatch-discrimination&quot;&gt;Minibatch discrimination&lt;/h3&gt;

&lt;p&gt;GAN에서 흔하게 나타나는 현상이 mode collapsing으로 D가 제일 헷갈리는 sample 하나만 G가 생성하는 것이다. 이 논문에서는 이러한 현상을 막기 위해서 D가 여러 샘플을 보게 하는 학습 기법을 &lt;em&gt;minibatch discrimination&lt;/em&gt;이라고 칭한다.&lt;/p&gt;

&lt;p&gt;구체적인 학습 방법은 이렇다. $\mathbf{f(x_{i})} \in \mathbb{R}^{A}$에 matrix $T \in \mathbb{R}^{A \times B \times C}$를 곱힌 결과를 $M_{i} \in \mathbb{R}^{B \times C}$라고 하자. 이 $M_{i}$의 행들간의 $L$-1 distance를 구하고 그 값에다가 negative exponential을 취한 값을 모두 합해서 학습에 사용한다.&lt;/p&gt;

&lt;p&gt;$\sigma(x_{i, b})= \sum_{j=1}^{n} c_{b}(x_{i}, x_{j}) = \sum_{j=1}^{n} \exp(- \vert\vert M_{i, b} - M_{j, b} \vert\vert)\in \mathbb{R}$&lt;/p&gt;

&lt;p&gt;$\sigma(x_{i}) = [\sigma(x_{i, 1}), …, \sigma(x_{i, B})] \in \mathbb{R}^{B}$&lt;/p&gt;

&lt;p&gt;$\sigma(\mathbb{X}) \in \mathbb{R}^{n \times B}$&lt;/p&gt;

&lt;p&gt;$\sigma(\mathbb{x_{i}})$를 $\mathbf{f(x_{i})}$에 concat해서 D의 next layer에 정보를 전달한다. 따라서 D는 현재 판단하는 sample뿐만 아니라 minibatch를 &lt;em&gt;side information&lt;/em&gt;으로 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;historical-averaging&quot;&gt;Historical averaging&lt;/h3&gt;

&lt;p&gt;이 기술은 cost에 $\vert\vert \mathbf{\theta} - \frac{1}{t}\sum_{i=1}^{t}\mathbf{\theta}[i]\vert\vert^{2}$ term을 포함하는 것이다. 이렇게 함으로써 online-learning의 효과를 갖게 되고 전에 배웠던 학습 정보를 잃지 않게 된다.&lt;/p&gt;

&lt;h3 id=&quot;one-sided-label-smoothing&quot;&gt;One-sided label smoothing&lt;/h3&gt;

&lt;p&gt;이 발상은 positive targets 갯수가 $\alpha$개, negative targets 갯수가 $\beta$개 일때 optimal D가 $D(x) = \frac{\alpha p_{data}(x) + \beta p_{model}(x)}{p_{data}(x) + p_{model}(x)}$가 되고 $p_{data}$가 0이고 $p_{model}$이 매우 클 때 G가 제대로 학습이 되지 않는다는 점에서 착안했다. 따라서 positive labels만 $\alpha$, negative labels는 0으로 smoothing 한다.&lt;/p&gt;

&lt;h3 id=&quot;virtual-batch-normalization&quot;&gt;Virtual batch normalization&lt;/h3&gt;

&lt;p&gt;DCGAN에서 batch normalization이 GAN의 성능을 크게 향상시킨다고 입증했지만 batch에 따라 성능이 크게 좌우된다. 따라서 학습을 시작할 때 &lt;em&gt;reference batch&lt;/em&gt;를 만들어서 &lt;em&gt;reference batch&lt;/em&gt;에 collect된 통계량을 통해 정규화를 진행한다.&lt;/p&gt;

&lt;h2 id=&quot;assessment-of-image-quality&quot;&gt;Assessment of image quality&lt;/h2&gt;

&lt;p&gt;원래는 GAN의 성능을 측정하기 위해 human anotators를 사용했는데, 이 논문에서는 &lt;strong&gt;Inception module&lt;/strong&gt; 이라는 human evaluation과 correlation이 높은 새로운 대안을 제시한다.&lt;/p&gt;

&lt;p&gt;우리는 우리 model이 $p(y \vert x)$는 low entropy를 갖고, $\int p(y \vert x = G(z))dx$가 high entropy를 갖기를 기대한다. 따라서 $\exp (\mathbb{E_{x}} KL(p(y \vert x) \vert\vert p(y)))$를 새로운 metric으로 제시한다. 막상 이 metric을 학습에 사용해서는 큰 효과를 보지 못했지만, human evaluation과 human judgement와 큰 양의 상관관계가 있다고 한다.&lt;/p&gt;

&lt;h2 id=&quot;semi-supervised-learning&quot;&gt;Semi-supervised learning&lt;/h2&gt;

&lt;p&gt;image를 K개의 클래스로 분류하는 classifier를 생각해보자. 이 모델의 마지막 레이어에서는 K-dimensional logit이 아웃풋이 된다. 즉, $[l_{1}, …, l_{K}]$을 통해 $p(y=j \vert x) = \frac{\exp (l_{j})}{\sum_{k=1}^{K} \exp(l_{k})}$를 구한다.&lt;/p&gt;

&lt;p&gt;여기서, G가 생성해낸 새로운 클래스를 $K+1$로 명명하고 classifier는 $p_{model}(y=K+1 \vert x)$까지 구분하도록 바뀐다. &lt;strong&gt;이로써 classifier는 unlabeled data(G에 의해 생성된 데이터)까지 활용하여 학습할 수 있다.&lt;/strong&gt; 새로운 Loss function은 아래처럼 바뀐다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/gans/new-loss.png&quot; alt=&quot;new loss&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 $L_{unsupervised}$는 G가 그럴듯한 이미지를 생성할 때만 도움이 된다. 이를 강제하는 방식은 D를 classifier로 바꾸고 G, D를 동시에 학습시키는 것이다. 이 방식으로 학습된 G를 통해 생성된 이미지는 human annotators로부터 좋은 평가를 받았다고 한다. 필자들은 본능적으로 사람이 이미지를 보았을 때 특정한 카테고리로 분류한다는 점에서 이런 결과가 나왔다고 설명한다. D로 하여금 &lt;em&gt;objectness&lt;/em&gt;를 측정하게 하는 것이다.&lt;/p&gt;

&lt;p&gt;위에 열거한 기법들을 통해 필자들은 큰 발전을 이루었다고 말하고 있다. 물론 이 기법들은 대부분 empirical results에 의존하고, 마땅히 이렇다할 이론적 근거들은 없다. 하지만 minibatch discrimination 같은 기법은 GAN 뿐만 아니라 다른 neural network에도 사용될 수 있을 것 같다.&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2020/12/27/Improved-Techniques-for-Training-GANs.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2020/12/27/Improved-Techniques-for-Training-GANs.html</guid>
        
        <category>GAN</category>
        
        <category>deep-learning</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>from Variational Inference to VAE</title>
        <description>&lt;p&gt;이번 포스팅에서는 Bayesian의 중요한 토픽인 Variational Inference부터 그와 연관된 Variational Auto-encoder까지 알아보려고 한다.&lt;/p&gt;

&lt;h1 id=&quot;bayesian-framework&quot;&gt;Bayesian Framework&lt;/h1&gt;

&lt;p&gt;먼저, 베이지안의 기본적인 사고방식부터 알고 가자. 빈도론자와 베이지안의 가장 큰 차이점은 우리가 추정하고자하는 parameter를 확률변수로 보냐/아니냐이다. 빈도론자는 고정된 상수라고 보고 베이지안은 어떤 확률분포를 따르는 확률변수라고 생각한다.&lt;/p&gt;

&lt;p&gt;예를 들어, 우리나라 사람들의 키를 수집한 데이터가 있다고 가정하자. 아무래도 우리가 관심있어 하는 parameter는 우리나라 사람들의 평균 키일 것이다. 빈도론자들은 이 평균 키($\mu$라고 하자.)가 고정된 상수(ex: 168cm)라고 가정하고 ML 방식으로 모수를 추정한다. 그에 비해, 베이지안은 $\mu$에 대한 사전 분포를 먼저 정의한 후(이를 prior belief라고 한다.) 주어진 데이터로 부터 사후분포를 추정한다.&lt;/p&gt;

&lt;p&gt;우리나라 사람들의 평균 키가 매우 작다고 믿는 베이지안은 평균이 160cm인 정규분포를 사전분포로 가정할 것이다. 그런데 데이터에 180cm 이상인 사람들이 많다면 데이터를 본 이후 사후분포는 평균이 175cm 정도인 정규분포가 된다. 한편, 빈도론자는 평균 키는 185cm구나!라고 결론을 지을 것이다.&lt;/p&gt;

&lt;p&gt;글이 길어졌는데, 여튼 베이지안의 핵심은 &lt;strong&gt;데이터에 사전믿음을 결합한다는 것&lt;/strong&gt;에 있다.&lt;/p&gt;

&lt;p&gt;우리가 추정하고자 하는 모수를 $\theta$, 데이터를 $x$라고 할 때 결국 &lt;strong&gt;베이지안의 목표는 사전분포 + 데이터로부터 사후분포를 추론하는 것이다.&lt;/strong&gt; 즉, 다음과 같다.&lt;/p&gt;

&lt;p&gt;$p(\theta\vert X) = \frac{\prod_{i=1}^{n}{p(x_{i}\vert\theta)p(\theta)}}{\int {\prod_{i=1}^{n}{p(x_{i}\vert\theta)p(\theta)d\theta}}} \text{ where } x_{i}’s \text{ are i.i.d samples}$&lt;/p&gt;

&lt;p&gt;그럼 이제 본격적으로 베이지안 입장에서 본 머신러닝 모델에 대해서 이야기해보도록 하자. $x$를 features, $y$를 class label/latent vector, $\theta$를 추정할 parameter로 정의하겠다. 그렇다면 우리가 관심있는 분포는 $x$가 given일 때 $y, \theta$의 결합 분포에 해당한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$p(y, \theta \vert x) = p(y \vert \theta, x)p(\theta) \text{ } \because x \perp\theta$&lt;/li&gt;
  &lt;li&gt;$p(\theta \vert X, Y) = \frac{p(Y \vert X, \theta)p(\theta)}{\int p(Y \vert X, \theta)p(\theta)}\text{ where X, Y denote whole training set}$&lt;/li&gt;
  &lt;li&gt;test: $p(y \vert x, X, Y) = \int{ p(y \vert x, \theta)p(\theta \vert X, Y)d\theta}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 바로 여기에서 문제가 생긴다.  $p(\theta \vert X, Y)$를 구하기 위해서는 분모에 있는 적분이 가능해야 하는데, 
$p(y \vert x, \theta)$와 $p(\theta)$가 conjugate하지 않으면 대부분의 경우에서 적분이 어렵다는 것. test시에도 마찬가지.&lt;/p&gt;

&lt;p&gt;(&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Conjugate_prior&quot;&gt;conjugate prior&lt;/a&gt;&lt;/strong&gt;: conjugacy에 대해서는 자세히 언급하지 않겠지만, 궁금하신 분들은 이 링크를 참조하길 바란다. 대표적인 conjugate distributions은 beta-binom, poission-gamma 등이 있다.)&lt;/p&gt;

&lt;p&gt;여튼, conjugacy가 없다면 posterior distribution(사후분포)을 구하기가 매우 힘들고 빈도론자들 처럼 $\theta$에 대한 point estimation을 할 수 밖에 없다. 이 경우를 Poor Bayes라고도 한다고… test시에도 이러한 point estimation을 통해 얻어진 $\theta_{MP}$를 가지고 $y$에 대한 추론을 하게 된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\theta_{MP} = argmax_{\theta}p(\theta \vert X, Y) = argmax_{\theta}P(Y \vert X, \theta)p(\theta)$&lt;/li&gt;
  &lt;li&gt;$p(y \vert x, X, Y) \approx p(y \vert x, \theta_{MP})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;덧붙여서 말하자면, 빈도론자들이 overfitting을 막기 위해 쓰는 regularization 기법(ex: L2-loss)가 사실 이 Poor Bayes와 본질적으로 동등하다.&lt;/p&gt;

&lt;h1 id=&quot;variational-inference&quot;&gt;Variational Inference&lt;/h1&gt;
&lt;h2 id=&quot;main-goal-to-estimate-ptheta-vert-x&quot;&gt;Main Goal: to estimate $p(\theta \vert x)$&lt;/h2&gt;

&lt;p&gt;그러면 conjugacy가 없고, 다시 말해 analytical하게 푸는 것이 불가능한 상황에서 우리는 어떻게 해야할까? 방법은 크게 두 가지로 나뉜다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;variational inference&lt;/strong&gt;: $q(\theta) \approx p(\theta \vert x)$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sampling based method&lt;/strong&gt;: $p(x \vert \theta)p(\theta)$로 부터 샘플링하는 방법. MCMC 등이 있으나 시간이 오래 걸린다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;우리는 여기서 첫 번째 방법인 variational inference에 대해 알아보려고 한다. approximate posterior를 가정하고, true posterior과 최대한 가깝게 approximate posterior를 추정하는 방법이다. 분포의 거리를 측정하기 위해 우리는 KL-divergence를 사용한다. KL-divergence는 워낙 유명한 토픽이고 서치하기도 쉬우니까 생략..&lt;/p&gt;

&lt;p&gt;$\hat{q}(\theta) = argmin_{q}D_{KL}(q(\theta) \vert\vert p(\theta \vert x)) = argmin_{q} \int q(\theta)log\frac{q(\theta)}{p(\theta \vert x)}d\theta$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;문제1: $p(\theta \vert x)$를 모른다.&lt;/li&gt;
  &lt;li&gt;문제2: 분포에 대한 optimization은 어떻게 할 수 있나?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Sol)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$logp(x) = E_{q(\theta)}[logp(x)] = \int q(\theta)logp(x)d\theta = \int q(\theta)log\frac{p(x, \theta)}{p(\theta \vert x)}d\theta= \int q(\theta)log\frac{p(x, \theta)}{p(\theta \vert x)}\frac{q(\theta)}{q(\theta)}d\theta$&lt;/p&gt;

&lt;p&gt;$= \int q(\theta) log\frac{p(x, \theta)}{q(\theta)}d\theta + \int q(\theta) log\frac{q(\theta)}{p(\theta \vert x)}d\theta = \mathcal{L}(q(\theta)) + D_{KL}(q(\theta) \vert\vert p(\theta \vert x))$&lt;/p&gt;

&lt;p&gt;따라서, $D_{KL}(q(\theta) \vert\vert p(\theta \vert x))$를 minimize하는 문제는 $\mathcal{L}(q(\theta))$를 maximize하는 문제와 동등해진다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L}(q(\theta)) = \int q(\theta) log\frac{p(x, \theta)}{q(\theta)}d\theta = \int q(\theta) log\frac{p(x \vert \theta)p(\theta)}{q(\theta)}d\theta$&lt;/p&gt;

&lt;p&gt;&lt;b&gt;&lt;font color=&quot;red&quot;&gt;$= E_{q(\theta)}[logp(x \vert \theta)] - D_{KL}(q(\theta) \vert\vert p(\theta)) = \text{data likelihood + KL-regularizer term}$&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;이제 남은 부분은 $q(\theta)$를 어떻게 최적화하는지인데, 크게 두 가지 방법이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Variational_Bayesian_methods&quot;&gt;mean field approximation&lt;/a&gt;&lt;/strong&gt;: $\theta$끼리 독립일 때 사용하는 방법.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;parametric approximation&lt;/strong&gt;: 대부분의 neural network에서 사용하는 방법. $q(\theta)=q(\theta \vert \lambda)$라고 정의한 후 $\lambda$에 대해서 최적화.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;지금까지 배운 것들을 요약해보자면 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Full Bayesian inference: $p(\theta \vert x)$&lt;/li&gt;
  &lt;li&gt;MP inference: $\theta_{MP} = argmax_{\theta}p(\theta \vert X, Y)$&lt;/li&gt;
  &lt;li&gt;Mean field variational inference:  $p(\theta \vert x) \approx q(\theta) = \prod_{j=1}^{m}q_{j}(\theta_{j})$&lt;/li&gt;
  &lt;li&gt;Parametric variational inference: $p(\theta \vert x) \approx q(\theta) = q(\theta \vert \lambda)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;latent-variable-models&quot;&gt;Latent Variable Models&lt;/h1&gt;

&lt;p&gt;그럼 VAE를 배우기 전에 먼저 latent variable models에 대해서 짚고 넘어가자. variational inference에 대해서 신나게 공부하다가 갑자기 잠재변수모델이라니 조금 뜬금없어보이지만 VAE는 잠재변수 모델의 일종이기 때문에 반드시 짚고 넘어가야 한다.&lt;/p&gt;

&lt;p&gt;왜 &lt;strong&gt;잠재변수&lt;/strong&gt;를 학습해야하는가? 이미지 데이터를 예로 들어보자. RGB 채널을 갖는 32x32 짜리 이미지 데이터는 32x32x3 = 3072 차원을 갖는다. 그러나 통상적으로 생각해보았을때, 3072 차원을 통째로 다 feature로 쓰기 보다는 이미지를 결정하는 잠재변수가 있다고 보고 이를 바탕으로 추론을 하는 것이 타당하다.&lt;/p&gt;

&lt;p&gt;예를 들어 MNIST 데이터에서 28x28=784개의 픽셀이 모두 의미있는 값이라고 보기보다는 숫자의 모양을 결정하는 변수(가장자리의 빈 정도, 선의 굽은 모양 등)가 있다고 보는 것이 맞다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png&quot; alt=&quot;MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;잠재변수 모델을 설명하는데 가장 흔하게 쓰이는 분포가정이 &lt;strong&gt;Mixture of Gaussians&lt;/strong&gt;이다. 즉, 여러개의 가우시안 분포가 혼합되어 있는 분포로 아래 그림과 같다.앞서 말한 대한민국 평균 키로 설명해보자면, 우리나라 사람들의 키의 분포는 남성/여성/성인/아동 등 여러 분포로 나뉠 수 있다.  (&lt;a href=&quot;https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95&quot;&gt;이미지 출처&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1200/1*lTv7e4Cdlp738X_WFZyZHA.png&quot; alt=&quot;Mixture of Gaussians&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 $i$번째 표본을 $x_{i}$라고 하고 그 표본이 속한 집단을 $z_{i}$(잠재변수)라고 해보자. 그러면 우리가 가진 데이터의 likelihood는 다음과 같이 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;$p(X, Z \vert \theta)=\prod_{i=1}^{n}p(x_{i}, z_{i} \vert \theta) = \prod_{i=1}^{n}p(x_{i} \vert z_{i},\theta)p(z_{i} \vert \theta) = \prod_{i=1}^{n}\pi_{z_{i}} \mathcal{N}(x_{i} \vert \mu_{z_{i}}, \sigma_{z_{i}}^{2})$&lt;/p&gt;

&lt;p&gt;여기서 $\pi_{j}=p(z_{i}=j)$로 $j$번째 그룹에 속할 확률을 의미하고 추정해야 할 파라미터는 $\theta = ( \mu_{j}, \sigma_{j}, \pi_{j} )_{j=1}^{K}$를 뜻한다.&lt;/p&gt;

&lt;p&gt;만약 $X, Z$를 모두 안다면 $\hat{\theta} = argmax_{\theta}logP(X, Z \vert \theta)$로 쉽게 추정할 수 있겠지만 &lt;strong&gt;문제는 우리는 Z를 모른다는 것이다. 따라서 우리는 $X$의 log likelihood를 최대화&lt;/strong&gt;하게 되고 목표식은 아래와 같다.&lt;/p&gt;

&lt;p&gt;$logP(X \vert \theta)=\int q(Z)logP(X \vert \theta)dZ=\int q(Z) log \frac{P(X, Z \vert \theta)}{P(Z \vert \theta)} \frac{q(Z)}{q(Z)}dZ = \mathcal{L(q(Z))}+D_{KL}(q(Z) \vert\vert p(Z \vert \theta))$&lt;/p&gt;

&lt;p&gt;항상 KL-divergence는 0 이상이므로 $logP(X \vert \theta)$의 lower-bound는 $\mathcal{L}(q(Z))$가 된다. &lt;strong&gt;이를 Variational lower bound 또는 ELBO라고 칭한다.&lt;/strong&gt; 결국, 우리는 이 하한값을 maximize하는 $q, \theta$를 찾는 것으로 목표를 바꾸게 된다. &lt;font color=&quot;red&quot;&gt;결국, 잠재변수만 추가되었을 뿐 위에서 배운 variational inference와 완전히 똑같은 문제다!&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;이를 푸는 방법으로 &lt;strong&gt;EM 알고리즘&lt;/strong&gt;이 존재한다. EM은 Expectation-Maximization의 약자로, 이름 그대로 Expectation step과 Maximization step이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;E-step: $q(Z)$를 추론하는 과정으로, 이때 $\theta=\theta_{0}$으로 고정된다.&lt;br /&gt;
$q(Z) = argmax_{q}\mathcal{L}(q, \theta_{0}) = argmin_{q}D_{KL}(q(z) \vert\vert p(z \vert \theta))=p(Z \vert X, \theta_{0})$&lt;br /&gt;
자세히 풀어서 설명하자면 다음과 같다. $q(Z)$는 Multinomial 분포임을 기억하자.&lt;br /&gt;
$q(z_{i}=k)=p(z_{i}=k \vert x, \theta) = \frac{p(x_{i} \vert k, \theta)p(z_{i}=k \vert \theta)}{\sum_{l=1}^{K}p(x_{i} \vert l, \theta)p(z_{i}=l \vert \theta)}$&lt;/li&gt;
  &lt;li&gt;M-step: $q(Z)$를 고정시켜놓고 $\theta$를 추론하는 과정이다.&lt;br /&gt;
$\hat{\theta} = argmax_{\theta} \mathcal{L}(q, \theta) = argmax_{\theta} \mathbb{E_{Z}}[logp(X, Z \vert \theta)]=\sum_{i=1}^{n}\sum_{k=1}^{K}q(z_{i}=k)logp(x_{i}, k \vert \theta)$&lt;/li&gt;
  &lt;li&gt;repeat 1, 2 until convergence.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;자, 여기서 드는 의문점이 있다. 위의 상황에서는 $Z$가 categorical variable이니까 단순합으로 E-step에서 $P(Z \vert X, \theta)$를 계산할 수 있다. &lt;strong&gt;하지만 $Z$가 만약 continuous variable이라면? $p(x \vert z, \theta), p(z \vert \theta)$가 conjugate 하지 않다면 intractable 하게 된다!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;continuous latent variable을 학습하는 것은 dimension reduction(차원축소) 또는 &lt;strong&gt;representation learning&lt;/strong&gt;에 해당하고 사실 머신러닝에서 매우매우 중요하면서도 어려운 부분이다. 적분으로 인한 intractable 문제를 VAE에서는 어떻게 해결하는지 다음 섹션에서 알아보겠다.&lt;/p&gt;

&lt;h1 id=&quot;stochastic-variational-inference-and-vae&quot;&gt;Stochastic Variational Inference and VAE&lt;/h1&gt;

&lt;p&gt;우리는 지금까지 Bayesian framework를 이용한 variational inference와 latent variable model에 대해서 배웠다. 실제로 관측되지 않는 잠재변수를 모델링하기 위해 variational inference를 사용($q(Z)$를 추론)해 학습을 진행하는 방법이었다. 하지만 사후분포를 추론할 때 처럼 잠재변수 $Z$가 continuous 하다면 intractability 문제에 직면하게 된다. 앞서 잠깐 언급한 바와 같이 이 문제를 해결하기 위해 여러 sampling 방법들이 고안되었다. 하지만 역시 시간이 많이 걸린다. 또한 Monte Carlo로 추정한 gradient는 분산이 매우 커진다고 한다. &lt;strong&gt;이런 한계점을 극복하기 위해 VAE는 reparameterization trick을 이용하였고, end-to-end learning이 가능해졌다!&lt;/strong&gt;&lt;/p&gt;

&lt;font color=&quot;red&quot;&gt;지금까지와 다르게, VAE는 generative model인 동시에 representaion learning을 학습하는 모델인 것을 기억하자.&lt;/font&gt;
&lt;p&gt;즉, 우리의 목표는 두 가지다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generation을 제대로 할 것 =&amp;gt; $logP(X)$를 maximize하는 목표&lt;/li&gt;
  &lt;li&gt;Latent variable Z의 분포를 제대로 학습할 것 =&amp;gt; $q(Z \vert X) \approx p(Z \vert X)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;먼저, 첫 번째 목표를 이루기 위해 $logP(X)$를 풀어쓰면 다음과 같다. (&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf&quot;&gt;이미지 출처&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbo6sRJ%2FbtqM4yIGX6T%2FjfBk3Mab5Dx4KFsi8QHeZk%2Fimg.png&quot; alt=&quot;logP(X)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금까지와 같이, 맨 마지막 KL-term을 제외한 나머지 것들이 lower bound가 된다. &lt;strong&gt;결국, $logP(X)$를 최대화하는 목표는 lower bound를 최대화하는 목표로 바뀌고 이는 동시에 두 번째 목표까지 이루게 된다!&lt;/strong&gt; lower bound 식은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$\mathcal{L}(\theta, \phi; x^{(i)}) = D_{KL}(q(z \vert x^{(i)}) \vert\vert p(z))+\mathbb{E_{q(z \vert x^{(i)})}}[logp(x^{(i)} \vert z)]$&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;앞부분은 prior과 approximate posterior와의 KL term이고, 뒷부분은 decoder probability에 해당한다.&lt;/strong&gt; 대부분 잠재변수 Z의 prior 분포를 $\mathcal{N}[0, 1]$와 같은 다루기 쉬운 분포로 정한다. 그러면 $q(z \vert x)$는 어떻게 정의했을까? VAE original paper에서는 다변량 정규분포로 정의하는데, 다음과 같다.&lt;/p&gt;

&lt;p&gt;$q(z_{i} \vert x_{i}, \phi) = \prod_{j=1}^{d}\mathcal{N}[\mu_{j}(x_{i}), \sigma_{j}^{2}(x_{i})]$&lt;/p&gt;

&lt;p&gt;이때 $\mu_{j}(x_{i}), \sigma_{j}^{2}(x_{i})$는 $x_{i}$가 DNN을 통과한 output에 해당한다. 그래서 구현된 코드를 보면 알겠지만, VAE의 encoder에서는 $\mu_{j}(x_{i}), \sigma_{j}^{2}(x_{i})$를 구한다. 그러면 $p(z), q(z \vert x)$의 KL-divergence를 구할 수 있게 된다. (둘 다 정규분포이므로) &lt;strong&gt;사실 이 term은 approximate posterior가 prior와 너무 달라지지 않게 하는 regularizer 역할을 해준다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;decoder probability에 해당하는 뒷부분을 보면 $q(z \vert x)$에 기반하여 $log(x \vert z)$의 평균을 구해야 한다. 바로 여기서 intractability에 직면한다. 앞서 말했다시피 Monte Carlo 방법으로 평균을 추정하게 되면 gradient의 분산이 매우 커지는 동시에 수렴할 때까지 시간이 오래걸리는 문제가 있다. &lt;strong&gt;게다가 무엇보다도, sampling은 미분가능한 연산이 아니기 때문에 역전파로 학습할 수가 없게 된다.&lt;/strong&gt; VAE의 저자들을 똑똑하게도, &lt;strong&gt;reparameterization trick&lt;/strong&gt;을 이용했다.&lt;/p&gt;

&lt;font color=&quot;red&quot;&gt;$q_{\phi}(z \vert x) \rightarrow g(\epsilon, x)$
&lt;/font&gt;

&lt;p&gt;사실 이 수식이 reparam trick의 전부인데, 처음에는 수식만 보고 읭?했었다. 그런데 회귀분석의 문제로 이해하면 쉬운 문제다.&lt;/p&gt;

&lt;p&gt;간단하게 언급하자면, $y$변수(타겟변수)가 $x$변수(feature)와 linear한 관계에 있다고 가정하고 $y = ax+b+\epsilon$식에서 $a, b$를 푸는 것인데 결국 이는 $p(y \vert x)$를 구하는 태스크가되고 $x$는 given, $a, b$는 constant라고 가정하기 때문에 random factor은 $\epsilon$ ~ $N(0, 1)$에서만 생긴다. 즉, $p(y \vert x)$는 $ax+b$를 평균으로하고 1을 분산으로 하는 정규분포가 된다. 따라서 $a, b$는 MLE 방법으로 closed-form solution이 나오게 된다. 지금까지 설명한 VAE와 개념적으로 상당히 비슷함을 알 수 있다.&lt;/p&gt;

&lt;p&gt;결국 $g(\epsilon, x)$는 본인은 deterministic한 function인데 외부에서 noise $\epsilon$이 들어왔다고 이해하게 되고, 미분이 가능해진다. &lt;strong&gt;end-to-end learning이 가능해지는 것이다!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 VAE의 단점인 blurry generation을 짚고 넘어가려고한다. approximate posterior가 regularizer 역할을 하고, reconstruction loss가 실제 cost에 해당한다고 볼 수 있기 때문에 $logp(x \vert z)$를 높이는 방향으로 학습이 된다. 이는 일종의 Linear Regression(MLE)으로 볼 수 있고, 결국 $x$의 평균과 가까워지게 된다. 따라서 VAE로 생성된 이미지는 보다 흐리다.&lt;/p&gt;

&lt;p&gt;VAE로 학습된 Z를 통해 이미지를 생성한 결과는 다음과 같다. (&lt;a href=&quot;https://arxiv.org/pdf/1312.6114.pdf&quot;&gt;이미지 출처&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/vae/vae.png&quot; alt=&quot;vae&quot; /&gt;&lt;/p&gt;

&lt;p&gt;D=2인 Z축에서 매우 smooth하게 변하고 있음을 볼 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;이번 포스팅에서는 Bayesian의 중요한 토픽인 Variational Inference부터 그와 연관된 Variational Auto-encoder까지 알아보았다. intractible posterior를 estimate하기 위한 기법 중의 하나가 Variational Inference였고 EM 알고리즘 등을 통해 잠재변수 모델에 활용됨을 알 수 있었다. VAE는 이를 활용한 생성모델+잠재변수 모델로 보다 시각화/설명이 용이하지만 흐린 이미지를 생성한다는 것까지 살펴보았다. 앞으로도 representation learning의 중요성은 더 부각될 것 같다. 열심히 공부해야지..&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2020/12/22/from-Variational-Inference-to-VAE.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2020/12/22/from-Variational-Inference-to-VAE.html</guid>
        
        <category>bayesian</category>
        
        <category>deep-learning</category>
        
        <category>variational-inference</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>Unsupervised Landmark Learning</title>
        <description>&lt;h1 id=&quot;unsupervised-learning-of-object-landmarks-through-conditional-image-generationhttpspapersnipsccpaper7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generationpdf&quot;&gt;&lt;a href=&quot;https://papers.nips.cc/paper/7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generation.pdf&quot;&gt;*1. Unsupervised Learning of Object Landmarks through Conditional Image Generation&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;task-generate-the-target-image-given-the-source-image-and-the-encoded-target-image&quot;&gt;Task: Generate the target image given the source image and the encoded target image.&lt;/h2&gt;

&lt;h2 id=&quot;method&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;heatmaps-bottleneck&quot;&gt;Heatmaps bottleneck&lt;/h3&gt;
&lt;p&gt;$\Phi(x)$: learn to extract keypoint-like structures&lt;/p&gt;

&lt;p&gt;$S_{u}(x;k)$: K heatmaps
=&amp;gt; $u_{k}^{*}(x) = \frac{\sum ue^{S_{u}(x;k)}}{\sum e^{S_{u}(x;k)}}$ (spatial softmax)&lt;/p&gt;

&lt;p&gt;$\Phi_{u}(x;k)$: Gaussian-like function centered at $u_{k}^{*}$&lt;/p&gt;

&lt;h3 id=&quot;generator-perceptual-loss&quot;&gt;Generator: Perceptual Loss&lt;/h3&gt;
&lt;p&gt;$\mathcal{L}(x’, \hat{x’}) = \sum_{l} \alpha_{l} \vert\vert\Gamma_{l}(x’)-\Gamma_{l}(\hat{x’})\vert\vert^{2}$: perceptual loss&lt;/p&gt;

&lt;h2 id=&quot;overall-architecture&quot;&gt;Overall Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/cond.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;By using K-heatmaps as a bottleneck having keypoints information and exploiting training method of conditional image generation, this paper achieved SOTA results of landmark detection&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-landmark-learning-from-unpaired-datahttpsarxivorgpdf200701053pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2007.01053.pdf&quot;&gt;*2. Unsupervised Landmark Learning from unpaired data&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;task-reconstructing-images-with-the-apprearance-and-pose-originated-from-different-images-and-establish-various-consistencies-among-these-reconstructed-images&quot;&gt;Task: Reconstructing images with the apprearance and pose originated from different images and establish various consistencies among these reconstructed images&lt;/h2&gt;

&lt;h2 id=&quot;method-1&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;cross-image-cycle-consistency-framework&quot;&gt;Cross-Image cycle Consistency Framework&lt;/h3&gt;
&lt;p&gt;Given $I_{i}, I_{j}$: pair of images&lt;/p&gt;

&lt;p&gt;$\mathbf {a_{i}} = E_{a}(N(I_{i})), \mathbf {a_{j}} = E_{a}(N(I_{j}))$&lt;/p&gt;

&lt;p&gt;$\mathbf {p_{i}} = E_{p}(N(I_{i})), \mathbf {p_{j}} = E_{p}(N(I_{j}))$&lt;/p&gt;

&lt;p&gt;$I_{i, j} = D(\mathbf {a_{i}}, \mathbf {p_{j}}), I_{j, i} = D(\mathbf {a_{j}}, \mathbf {p_{i}})$&lt;/p&gt;

&lt;p&gt;$\mathbf {a_{i}}’ = E_{a}(N(I_{i, j})), \mathbf {a_{j}}’ = E_{a}(N(I_{j, i}))$&lt;/p&gt;

&lt;p&gt;$\mathbf {p_{i}}’ = E_{p}(N(I_{i,j})), \mathbf {p_{j}}’ = E_{p}(N(I_{j,i}))$&lt;/p&gt;

&lt;p&gt;$I_{i, j}’ = D(\mathbf {a_{i}}’, \mathbf {p_{j}}’), I_{j, i}’ = D(\mathbf {a_{j}}’, \mathbf {p_{i}}’)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$\mathcal{L_{cycle}} = \mathcal{P}(I_{i}, I_{i}’)+\mathcal{P}(I_{j}, I_{j}’)$: P is perceptual loss implemented by VGG network&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$\mathcal{L_{inv}} = \vert\vert\mathbf {p_{i}}’ - \mathbf{p_{i}}\vert\vert ^ {2} + \vert\vert\mathbf {p_{j}}’ - \mathbf{p_{j}}\vert\vert ^ {2}$&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;regularization-via-cross-image-flow-module&quot;&gt;Regularization via Cross-Image Flow Module&lt;/h3&gt;

&lt;p&gt;$T^{i \rightarrow j}$: the location correspondences&lt;/p&gt;

&lt;p&gt;$\mathbf{C}$: 4D tensor containing the element-wise cosine similiarity between two feature maps($\mathbf {f_{i}}’, \mathbf {f_{j}}’$)&lt;/p&gt;

&lt;p&gt;$\hat{C} = \mathbf {W_{1}} \bigotimes (\mathbf {W_{2}} \bigotimes \mathbf {C})$: 4D convolution using 2D convolutions consequently&lt;/p&gt;

&lt;p&gt;$S^{i \rightarrow j}(x_{j}, y_{j}) = softmax(\hat{\mathbf{C}}(*, *, x_{j}, y_{j}))$&lt;/p&gt;

&lt;p&gt;$T^{i \rightarrow j}(x_{j}, y_{j}) = argmax_{(x_{i}, y_{i})}S^{i \rightarrow j}(x_{j}, y_{j})$ (vice versa for j-&amp;gt;i)&lt;/p&gt;

&lt;p&gt;Thus, transformation maps can reflect the semantic correlations between landmarks of two images.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$\mathcal{L_{equiv}} = \vert\vert\mathbf {p_{i}} - T^{j \rightarrow i}\circ\mathbf{p_{j}}\vert\vert ^ {2}+\vert\vert\mathbf {p_{j}} - T^{i \rightarrow j}\circ\mathbf{p_{i}}\vert\vert ^ {2}$&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;final-loss-mathcalltotallambdacyclemathcallcyclelambdaequivmathcallequivlambdainvmathcallinv&quot;&gt;Final Loss: $\mathcal{L_{total}}=\lambda_{cycle}\mathcal{L_{cycle}}+\lambda_{equiv}\mathcal{L_{equiv}}+\lambda_{inv}\mathcal{L_{inv}}$&lt;/h3&gt;

&lt;h2 id=&quot;overall-architecture-1&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/unpaired.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-1&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Through techinique of balancing losses which can be categorized into meaningful embedding/invariance/equivariance, unpaired images can be used to locate landmarks&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-part-based-disentangling-of-object-shape-and-appearancehttpsarxivorgpdf190306946pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1903.06946.pdf&quot;&gt;*3. Unsupervised Part-Based Disentangling of Object Shape and Appearance&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-2&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;part-based-representation&quot;&gt;Part-based Representation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;part based factorization of representation: $\phi(x) := (\phi_{1}(x), \phi_{2}(x), …)^{\top}$ where $\phi_{i}(x)$ can be decomposed as $[\alpha_{i}(x), \sigma_{i}(x)]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;invariance-and-equivariance&quot;&gt;Invariance and Equivariance&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;invariance: $i)\alpha_{i}(x \circ s)=\alpha_{i}(x)$, $ii)\sigma_{i}(a(x))=\sigma_{i}(x)$ =&amp;gt; &lt;strong&gt;$\mathcal {L_{rec}} = \vert\vert x - D([\alpha_{i=1,…}(x), \sigma_{i=1,…}(x)]) \vert\vert_{1}$&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;equivariance: $\sigma_{i}(x \circ s) = \sigma_{i}(x) \circ s$ =&amp;gt; &lt;strong&gt;$\mathcal {L_{equiv}} = \sum_{i} \lambda_{\mu} \vert\vert \mu[\sigma_{i}(x \circ s)] - \mu[\sigma_{i}(a(x))\circ s]\vert\vert_{2} + \lambda_{\sum} \vert\vert \sum[\sigma_{i}(x \circ s)] - \sum[\sigma_{i}(a(x))\circ s]\vert\vert_{1}$&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;total loss: $\mathcal {L} = \mathcal {L_{rec}} + \mathcal {L_{equiv}}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overall-architecture-2&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/shape-appear.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where $f$ denotes localized image encoding&lt;/p&gt;

&lt;h2 id=&quot;summary-2&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;To sum up, shape stream extracts part shapes which are independent from appreances and appreance stream does same thing as well except it re-encodes part appearances using local features. In decoder, reconstruction is done by using approximate part shapes(normalized) and part appreances weighted on part shapes&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;self-supervised-learning-of-interpretable-keypoints-from-unlabelled-videoshttpswwwrobotsoxacukvggpublications2020jakab20jakab20pdf&quot;&gt;&lt;a href=&quot;https://www.robots.ox.ac.uk/~vgg/publications/2020/Jakab20/jakab20.pdf&quot;&gt;*4. Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;task-recognizing-the-pose-of-objects-from-a-single-image-that-for-learning-uses-only-unlabelled-videos-and-a-weak-empirical-prior-on-the-objects-poses&quot;&gt;Task: Recognizing the pose of objects from a single image that for learning uses only unlabelled videos and a weak empirical prior on the objects poses&lt;/h2&gt;

&lt;h2 id=&quot;method-3&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;${y} = \Phi({x})$: pose extracted from image&lt;/li&gt;
  &lt;li&gt;$\Psi(\Phi(x), x’)$: conditional decoder network&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dual-representation-of-pose--bottleneck&quot;&gt;Dual representation of pose &amp;amp; bottleneck&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{p} = (p_{1}, …, p_{K})$: a vector of K 2D keypoint coordinates trained by $\eta(\mathbf {y})$&lt;/li&gt;
  &lt;li&gt;$y* = \beta(\mathbf{p})_{u}$: a distance field from line segments that forms the skeleton image&lt;/li&gt;
  &lt;li&gt;$x=\Psi(\beta \circ \eta \circ \Phi(x), x’)$: so as to prevent cheating by exploting dual representation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{L_{perc}}=\frac{1}{N}\sum_{i=1}^{N}{\vert\vert \Gamma(\hat{x_{i}}) - \Gamma(x_{i}) \vert\vert ^ {2}}$: &lt;strong&gt;Auto-encoding&lt;/strong&gt; loss implemented by perceptual loss&lt;/li&gt;
  &lt;li&gt;$\mathcal{L_{disc}}(D) = \frac{1}{M}\sum_{j=1}^{M} D(\bar{y_{j}})^{2} + \frac{1}{N}\sum_{i=1}^{N}{(1-D(y_{i}))^{2}}$: &lt;strong&gt;Difference adversarial loss&lt;/strong&gt; to match $p(y)\approx q(y)$, encourages the images $y$ ro be ‘skeleton-like’&lt;/li&gt;
  &lt;li&gt;$\mathcal{L}(\Phi, \Psi, D) = \lambda_{disc}\mathcal{L_{disc}}(D)+\mathcal{L_{perc}}(\Psi, \Phi)$: &lt;strong&gt;Total Loss&lt;/strong&gt;, minimized w.r.t $\Phi, \Psi$ and maximised w.r.t $D$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overall-architecture-3&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/pose.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-3&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;While using conditional image generator as a decoder, the method utilized in this paper forces encoder to grab meaningful pose information by exploiting dual representation and empirical prior&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-learning-of-landmarks-by-descriptor-vector-exchangehttpsarxivorgpdf190806427pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.06427.pdf&quot;&gt;5. Unsupervised Learning of Landmarks by Descriptor Vector Exchange&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-4&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;equivariance constraint: $\Phi_{u}(x) = \Phi_{gu}(x) \text{ where } \Phi \text{ correspondes to dense embedding}$&lt;/li&gt;
  &lt;li&gt;probabilistic formulation: $p(v \vert u;\Phi, x, x’)= \frac{e^{&amp;lt;\Phi_{u}(x), \Phi_{v}(x)&amp;gt;}}{\int_{\Omega} e^{&amp;lt;\Phi_{u}(x), \Phi_{t}(x)&amp;gt;}dt}$&lt;/li&gt;
  &lt;li&gt;$\mathcal {L}(\Phi;x, x’, g) = E_{u, v}[\vert\vert v - gu \vert\vert]$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vector-exchangeability&quot;&gt;Vector exchangeability&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$x_{\alpha}$: auxiliary image that belongs to same-category in $u, v$&lt;/li&gt;
  &lt;li&gt;$\hat {\Phi_{u}(x\vert x_{\alpha})} = \int \Phi_{w}(x_{\alpha})p(w \vert u; \Phi, x, x_{\alpha})dw$&lt;/li&gt;
  &lt;li&gt;new probabilistic formulation: $p(v \vert u;\Phi, x, x’)= \frac{e^{&amp;lt;\hat{\Phi_{u}}(x), \Phi_{v}(x)&amp;gt;}}{\int_{\Omega} e^{&amp;lt;\hat{\Phi_{u}}(x), \Phi_{t}(x)&amp;gt;}dt}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overall-architecture-4&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/DVE.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-4&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;DVE captures dense embedding of an image which acts as an invariant descriptor vector by enforcing it’s robustness via intra-class variants(auxiliary images), in which it learns equivariance and intra-class invariance simultaneously&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-learning-of-facial-landmarks-based-on-inter-intra-subject-consistencieshttpsarxivorgpdf200407936pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.07936.pdf&quot;&gt;6. Unsupervised Learning of Facial Landmarks based on Inter-Intra Subject Consistencies&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-5&quot;&gt;Method&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Landmark Detector&lt;/em&gt;: using visual feature maps $S \in \mathbf{R}^{H \times W \times K}$,&lt;br /&gt;
the predicted $k$-th landmark location $u_{k}$ is weighted average of the spatial locations $i$&lt;br /&gt;
$\Phi_{H}(x;k) = \exp(-\frac{1}{2\sigma^{2}} \vert\vert u-u_{k} \vert\vert ^ {2})$: Gaussian-like probabilistic heatmap centered at $u_{k}$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Inter-Intra Image Generator&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;$x’, x^{a}$: deformed image, auxiliary image&lt;/li&gt;
      &lt;li&gt;$\mathcal {F_{s}} = \Phi_{E}(x) \in \mathbf {R^{H \times W \times D}}$: a visual feature map&lt;/li&gt;
      &lt;li&gt;$\mathcal {I_{a}} = \Psi(\mathcal {F_{s}}, \Phi_{H}(x^{a})) = \Psi(\Phi_{E}(x), \Phi_{H}(x^{a}))$&lt;/li&gt;
      &lt;li&gt;$\mathcal {I} = \Psi(\mathcal {F_{t}}, \Phi_{H}(x’)) = \Psi(\Phi_{E}(\mathcal {I_{a}}), \Phi_{H}(x’))$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Cycle Backward Path&lt;/em&gt;: both $X$ and $X’$ are reconstructed through it’s counterpart&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;reconstruction loss: $\mathcal {L_{R}}(\mathcal{I}, \mathcal{I_{gt}}) = \vert\vert \mathcal{I}-\mathcal{I_{gt}} \vert\vert ^{2}$&lt;/li&gt;
  &lt;li&gt;perceptual loss: $\mathcal {L_{P}}(\mathcal{I}, \mathcal{I_{gt}}) = \sum_{l} \vert\vert VGG^{l}(\mathcal{I}) - VGG^{l}(\mathcal{I_{gt}})\vert\vert ^ {2}$&lt;/li&gt;
  &lt;li&gt;total loss: $\mathcal{L} = \mathcal{L_{R}}(\mathcal{I_{x}}, x)+\mathcal{L_{R}}(\mathcal{I_{x’}}, x’)+\mathcal{L_{P}}(\mathcal{I_{x}}, x)+\mathcal{L_{P}}(\mathcal{I_{x’}}, x’)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overall-architecture-5&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/inter-intra.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-5&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;By inserting auxiliary image’s structure when reconstructing the target image, this method gives the model intra-subject consistency and reinforeces inter-subject consistency via cycle backward path&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-disentanglement-of-pose-appearance-and-background-from-images-and-videoshttpsarxivorgpdf200109518pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.09518.pdf&quot;&gt;7. Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos&lt;/a&gt;&lt;/h1&gt;

&lt;h1 id=&quot;unsupervised-discovery-of-object-landmarks-via-contrastive-learninghttpsarxivorgpdf200614787pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.14787.pdf&quot;&gt;*8. Unsupervised Discovery of Object Landmarks via Contrastive Learning&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-6&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;contrastive-learning&quot;&gt;Contrastive learning&lt;/h3&gt;
&lt;p&gt;The goal is to learn: $\langle\Phi(x), \Phi(x’)\rangle \gg \langle\Phi(x), \Phi(z)\rangle$&lt;/p&gt;

&lt;h3 id=&quot;traditional-methods&quot;&gt;Traditional methods&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Equivariant learning: enforces model to be robust to geometric/photometric transformations&lt;/li&gt;
  &lt;li&gt;Invariant learning: encourages the representations to be invariant to transformations while being distinctive across images&lt;/li&gt;
  &lt;li&gt;Trade off: Equivariant learning makes model to be less deeper(due to pooling operation), while invariant learning makes model to be more deeper(to capture more sophisticated features).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;novel-approach-incorporating-1-2&quot;&gt;Novel approach: incorporating 1, 2&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;$\mathcal{L_{NCE}} = -log\frac{\exp{(\langle \Phi(x), \Phi(x’) \rangle)}}{\sum_{i=1}^{N}\exp{(\langle \Phi(x), \Phi(x_{i}) \rangle)}}$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hypercolumns&lt;/strong&gt;: $\Phi_{u}(x) = \Phi_{u}^{k_{1}}(x) \bigoplus \Phi_{u}^{k_{2}}(x) … \bigoplus \Phi_{u}^{k_{n}}(x)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;overall-architecture-6&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/contrastive.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-6&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;In contrast to traditional approach that can be categorized into equivariant/invariant learning, the method used in this paper incorporates both approachs by using contrastive learning technique. To be specific, it trains invariant representations using constrastive learning and then extracts hypercolumn representation to detect landmarks.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;brul-barycenter-regularized-unsupervised-landmark-extractionhttpsarxivorgpdf200611643pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2006.11643.pdf&quot;&gt;9. BRULÉ: Barycenter-Regularized Unsupervised Landmark Extraction&lt;/a&gt;&lt;/h1&gt;

&lt;h1 id=&quot;unsupervised-learning-of-object-frames-by-dense-equivariant-image-labellinghttpsarxivorgpdf170602932pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1706.02932.pdf&quot;&gt;*10. Unsupervised learning of object frames by dense equivariant image labelling&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-7&quot;&gt;Method&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;invariant constraint: $\Phi(\mathbb{x}, u) = \Phi(g\mathbb{x}, gu)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Motivated by invariant constraints, the similarity $\langle \Phi(\mathbb{x}, u), \Phi(\mathbb{x’}, gu)\rangle$ should be larger than the similarity $\langle \Phi(\mathbb{x}, u), \Phi(\mathbb{x’}, v)\rangle$ where $g$ is an arbitrary optical flow module.&lt;/p&gt;

&lt;h3 id=&quot;loss-1&quot;&gt;Loss&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{L_{\log}}(\Phi \vert \mathbb{x}, \mathbb{x’}, g) = -\frac{1}{HW}\sum_{u}\log p(gu \vert u; \mathbb{x}, \mathbb{x’}, \Phi)$&lt;/li&gt;
  &lt;li&gt;$\mathcal{L_{\text{dist}}}(\Phi \vert \mathbb{x}, \mathbb{x’}, g) = \frac{1}{HW}\sum_{u}\sum_{v} \vert\vert v-gu \vert\vert_{2}^{\gamma} p(v \vert u; \mathbb{x}, \mathbb{x’}, \Phi)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overall-architecture-7&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/dense-labelling.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-learning-of-object-landmarks-by-factorized-spatial-embeddingshttpsarxivorgpdf170502193pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1705.02193.pdf&quot;&gt;*11. Unsupervised learning of object landmarks by factorized spatial embeddings&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-8&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;deformable-objects-equivariance&quot;&gt;Deformable objects: Equivariance&lt;/h3&gt;
&lt;p&gt;$\forall r \in S_{0} : \Phi(r;\mathbb{x} \circ g) = g(\Phi(r;\mathbb{x}))$&lt;/p&gt;
&lt;h3 id=&quot;semantically-consistent-network&quot;&gt;Semantically consistent network&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\Psi(\mathbb{x}) \in \mathbb{R^{H \times W \times K}}$: score maps&lt;/li&gt;
  &lt;li&gt;$p(u \vert \mathbb{x}, r) = \frac{e ^ {\Psi(\mathbb{x})}}{\sum_{v} e^{\Psi(\mathbb{x})}}$&lt;/li&gt;
  &lt;li&gt;$u_{r}^{*} = \sum_{u} up(u \vert \mathbb{x}, r)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;=&amp;gt; $\mathcal{L_{align}} = \frac{1}{K}\sum_{r=1}^{K}\sum_{uv} \vert\vert u-g(v) \vert\vert ^ {2}p(u \vert \mathbb{x}, r)p(v \vert \mathbb{x’}, r)$&lt;/p&gt;

&lt;h3 id=&quot;diversity-loss-penalize-the-mutual-overlap&quot;&gt;Diversity Loss: penalize the mutual overlap&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{L_{div}}(x) = \frac{1}{K^{2}} \sum_{r=1}^{K}\sum_{r’=1}^{K} \sum_{u}p(u \vert \mathbb{x}, r)p(u \vert \mathbb{x}, r’)$&lt;/li&gt;
  &lt;li&gt;$\mathcal{L_{div}’}(x)=K - \sum_{u} \max_{r=1,…,K}\sum_{\delta_{u}}p(mu+\delta_{u} \vert \mathbb{x}, r)$: $m \times m$ sum pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;total-loss&quot;&gt;Total Loss&lt;/h3&gt;
&lt;p&gt;$\mathcal{L_{total}} = \lambda\mathcal{R}(\Psi) + \frac{1}{N} \sum_{i=1}^{N}\mathcal{L_{align}’}(\mathbb{x}, \mathbb{x_{i}}, g_{i};\Psi) + \gamma \mathcal{L_{div}’’}(\mathbb{x_{i}};\Psi) + \gamma \mathcal{L_{div}’’}(\mathbb{x’_{i}};\Psi)$&lt;/p&gt;

&lt;h2 id=&quot;overall-architecture-8&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/spa.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-7&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;By factorzing deformations, we can learn intrinsic reference frame for the object&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;deforming-autoencoders-unsupervised-disentangling-of-shape-and-appearancehttpsarxivorgpdf180606503pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.06503.pdf&quot;&gt;12. Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance&lt;/a&gt;&lt;/h1&gt;

&lt;h1 id=&quot;self-supervised-learning-of-a-facial-attribute-embedding-from-videohttpsarxivorgpdf180806882pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1808.06882.pdf&quot;&gt;13. Self-supervised learning of a facial attribute embedding from video&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-9&quot;&gt;Method&lt;/h2&gt;
&lt;h3 id=&quot;multi-source-frames-architecture-predicting-a-confidence-heatmap&quot;&gt;Multi-source frames architecture: predicting a confidence heatmap&lt;/h3&gt;
&lt;h3 id=&quot;curriculum-strategy-stop-training-when-samples-fall-into-90th-100th-percentile-range&quot;&gt;Curriculum Strategy: stop training when samples fall into 90th-100th percentile range.&lt;/h3&gt;

&lt;h2 id=&quot;overall-architecture-9&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/facial-embeds.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-discovery-of-object-landmarks-as-structural-representationshttpsarxivorgpdf180404412pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.04412.pdf&quot;&gt;*14. Unsupervised Discovery of Object Landmarks as Structural Representations&lt;/a&gt;&lt;/h1&gt;

&lt;h2 id=&quot;method-10&quot;&gt;Method&lt;/h2&gt;

&lt;h3 id=&quot;architecture-of-landmark-detector&quot;&gt;Architecture of landmark detector&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{R} = hourglass_{l}(\mathbf{I};\theta_{l}) \in \mathbb{R}^{W \times H \times(K+1)}$: raw detection score map&lt;/li&gt;
  &lt;li&gt;$\mathbf{D_{k}}(u,v)$: normalized $\mathbf{R}$ across the channels
=&amp;gt; Taking $\mathbf{D_{k}}$ as a weighting map, $(x_{k}, y_{k})$ is the $k$-th landmark&lt;/li&gt;
  &lt;li&gt;$l = [x_{1}, y_{1}, …, x_{k}, y_{k}]^{\top} = \text{landmark}(\mathbf{I};\theta_{l})$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;visual-concept-of-landmarks&quot;&gt;Visual concept of landmarks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Concentration constraint: $\mathcal{L_{conc}} = 2\pi e (\sigma_{det, u}^{2}, \sigma_{det, v}^{2})$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;=&amp;gt; $\bar{D_{k}}(u, v)=(1/WH)\mathcal{N}((u, v);(x_{k}, y_{k}), \sigma_{det}^{2}\mathbb{I})$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Separation constraint: $\mathcal{L_{sep}} = \sum_{k \neq k’}^{1, …, K}\exp(-\frac{\vert\vert (x_{k’}, y_{k’}) - (x_{k}, y_{k}) \vert\vert ^{2}}{2\sigma_{sep}^{2}})$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Equivariance constarint: $\mathcal{L_{equiv}} = \sum_{k=1}^{K} \vert\vert g(x_{k}’, y_{k}’) - (x_{k}, y_{k}) \vert\vert ^ {2}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-latent-descriptors&quot;&gt;Local latent descriptors&lt;/h3&gt;

&lt;p&gt;$\mathbf{F} = hourglass_{f}(\mathbf{I};\theta_{f}) \in \mathbb{R}^{W \times H \times S}$&lt;/p&gt;

&lt;p&gt;Then take $\mathbf{f_{k}}$ as inner product of $\bar{D_{k}}(u, v), \mathbf{F}(u, v)$ multiplied by landmark-specific operator $\mathbf{W_{k}}$&lt;/p&gt;

&lt;h3 id=&quot;landmark-based-decoder&quot;&gt;Landmark-based decoder&lt;/h3&gt;

&lt;h3 id=&quot;total-loss-lambdareconlreconlambdaconclconclambdaseplseplambdaequivlequiv&quot;&gt;Total Loss: $\lambda_{recon}L_{recon}+\lambda_{conc}L_{conc}+\lambda_{sep}L_{sep}+\lambda_{equiv}L_{equiv}$&lt;/h3&gt;

&lt;h2 id=&quot;overall-architecture-10&quot;&gt;Overall Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/Unsupervised-Landmark/struct.png&quot; alt=&quot;model architecture&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;cross-domain-correspondence-learning-for-exemplar-based-image-translationhttpsarxivorgpdf200405571pdf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/2004.05571.pdf&quot;&gt;15. Cross-domain Correspondence Learning for Exemplar-based Image Translation&lt;/a&gt;&lt;/h1&gt;
</description>
        <pubDate>Sun, 20 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/review/2020/12/20/Unsupervised-Landmark-Learning.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/review/2020/12/20/Unsupervised-Landmark-Learning.html</guid>
        
        <category>deep-learning</category>
        
        <category>unsupervised-learning</category>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>DCGAN 리뷰</title>
        <description>&lt;p&gt;이 포스팅에서 리뷰할 DCGAN은 사실 GAN을 더욱 더 유명하게 만들어준 논문으로, 이 논문이 publish된 이후로 대부분의 GAN 아키텍쳐는 DCGAN의 아키텍쳐를 따른다.&lt;/p&gt;

&lt;p&gt;기존 CNN을 이용한 DNN에 architectural contraints를 더해줌으로써 hierarchical representation을 학습할 수 있는 것이 이 논문의 주요 내용이다.&lt;/p&gt;

&lt;p&gt;먼저, unsupervised learning의 의의를 설명해보자면 labeled dataset은 구하기 어려운 것에 비해 unlabeled dataset은 구하기가 매우 쉽다. 따라서 구하기 쉬운 대용량의 unlabeled dataset을 이용해 reusable한 feature representation을 학습할 수 있다면 supervised learning에도 유용히 쓰일 수 있고, 우리의 데이터의 분포를 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;이 논문의 proposal은 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;CNN을 이용한 GAN에 여러 constraints를 더 해줌으로써 GAN의 unstability 해결&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;학습된 discriminator를 이미지 분류 문제에 적용했더니 좋은 성능을 보임&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GAN의 필터를 시각화함으로써 explainability 확장&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;generator는 Word2vec처럼 흥미로운 vector arithmetic 성질이 존재&lt;/strong&gt; ex) king+woman =&amp;gt; queen&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면 해당 논문의 핵심이라고 할 수 있는 architectural constraints는 무엇일까? 사실 나는 굉장히 대단할 거라고 생각했는데 생각보다 간단해서 놀랐다..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/dcgan/1.png&quot; alt=&quot;architectural constraints&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;strided convolution: 이미지 본연의 spatial upsampling을 학습한다.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FC layer 대신 feature map을 flatten해서 사용&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Batch Normalization: mode collapsing 방지&lt;/strong&gt; (단, generator output layer와 discriminator input layer에는 적용하지 않음)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;결과적으로는 아래와 같은 구조가 된다.
&lt;img src=&quot;../../../../img/dcgan/dcgan.png&quot; alt=&quot;dcgan&quot; /&gt;&lt;/p&gt;

&lt;p&gt;100-dimension의 uniform distribution을 따르는 랜덤 벡터 Z를 4차원텐서(N x C x H x W)로 reshape 해준 다음 Transpose Convolution을 이용해서 계속 upsampling 해주고, 마지막엔 기본 RGB 이미지처럼 64x64x3으로 바꿔준다.&lt;/p&gt;

&lt;p&gt;이후론 학습에 사용된 여러 테크닉들이 나오는데, 대부분 generator가 학습 데이터를 그냥 외워버리는 것을 방지하거나/아님을 증명하기 위한 것들이다.&lt;/p&gt;

&lt;p&gt;작은 학습률(0.0002) + minibatch SGD를 이용하여 1 epoch 동안 학습을 진행했는데 성과가 좋았다는 것과 5 epoch 이후 오히려 underfitting이 일어났다는 것이 generator가 제대로 학습하고 있다는 증거에 해당한다.&lt;/p&gt;

&lt;p&gt;또한 개인적으로 인상깊었던 것은 duplication을 방지하기 위해 de-noising autoencoder를 이용해 이미지를 코드화한후 비슷한 코드끼리 제거했다는 것(semantic-hashing)이다.&lt;/p&gt;

&lt;p&gt;다음 섹션에서는 실제 데이터에 적용한 emprical validation을 열거한다. 앞서 말한 image classification task를 위해 discriminator의 모든 feature map을 사용하여 CIFAR-10을 분류한 결과 82.8%의 정확도를 달성했다고 한다.&lt;/p&gt;

&lt;p&gt;마지막 섹션에서는 generator가 학습한 representation의 validity를 입증한다. Z의 grid를 나눠서 이미지를 생성해보기도하고, discriminator의 필터를 시각화해보기도 하는데 제일 재미있었던 것은 특정 사물은 잊도록(forgetting) 해보는 실험이었다. 150개의 학습 데이터중 52개의 이미지에서 창문에 대한 bounding box를 수동으로 라벨링하고 generator의 마지막에서 두번째 convolution layer를 이용해 창문이 있는지/없는지 로지스틱 회귀를 fit한다. 이때 기준은 바운딩 박스안의 activation 값이 positive, 랜덤 이미지에서는 negative 값을 가지는지 여부로 정했다고 한다.&lt;/p&gt;

&lt;p&gt;이 모델을 이용해서 회귀계수 값이 0보다 큰 feature map은 drop한 후(random new samples로 대체)  generation을 진행한 결과는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../../../img/dcgan/4.png&quot; alt=&quot;windows&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 DCGAN 하면 제일 유명한 것 vector arthimetic properties에 관한 것일텐데.. 너무 유명하니까 생략하겠음..&lt;/p&gt;

&lt;p&gt;마지막으로는 future work에 대해 설명하는데, 학습을 오래 시킬수록 오히려 oscillating mode가 생긴다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;reference: &lt;a href=&quot;https://arxiv.org/pdf/1511.06434.pdf&quot;&gt;Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2020/12/20/DCGAN.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2020/12/20/DCGAN.html</guid>
        
        <category>DCGAN</category>
        
        <category>GAN</category>
        
        <category>deep-learning</category>
        
        <category>generative-model</category>
        
        
        <category>generative-models</category>
        
      </item>
    
      <item>
        <title>GAN 리뷰</title>
        <description>&lt;p&gt;오늘 리뷰할 논문은 GAN으로 더 잘 알려진, 적대적 인공신경망을 처음으로 제안한 논문이다.&lt;/p&gt;

&lt;p&gt;뭐 이미 말할 필요도 없이 유명하기도 하고, 수많은 variation이 나오기도 했다.&lt;/p&gt;

&lt;p&gt;논문의 제 1저자인 이안 굿펠로우는 GAN의 &lt;strong&gt;generator를 지폐위조범으로, discriminator를&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;경찰로 묘사했다.&lt;/strong&gt; 지폐위조범은 위조 지폐를 더욱 더 진짜 같이 위조하고, 경찰은 그걸 구분하려고&lt;/p&gt;

&lt;p&gt;경쟁하는 과정에서 상호발전이 일어난다는 것이다. 사실 우리의 목표는 경찰보다는&lt;/p&gt;

&lt;p&gt;지폐위조범의 생성능력을 상승시키는 것에 있다. 결국 학습의 끝에서는 경찰이&lt;/p&gt;

&lt;p&gt;어느 것이 위조 지폐인지 진짜 지폐인지 알아볼 수 없도록 말이다.&lt;/p&gt;

&lt;p&gt;이렇듯 기본적인 아이디어는 굉장히 직관적이지만, 이론적 배경은 꽤나 탄탄하다.&lt;/p&gt;

&lt;p&gt;이제 차근차근 하나씩 알아보겠다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;GAN을 알아보기 전, VAE에 대해서 먼저 잠깐 짚고 넘어가자.&lt;/p&gt;

&lt;p&gt;VAE의 가장 큰 문제점은 blurry한 이미지가 생성된다는 것이다. &lt;/p&gt;

&lt;p&gt;여러가지 설명이 있을 수 있겠지만, 필자가 생각하기로는 VAE의 loss function에서&lt;/p&gt;

&lt;p&gt;기인한다고 생각한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdC2FQH%2FbtqNQ4vjwsv%2F1j5oUsSiBtmeek9MJyFld0%2Fimg.png&quot; alt=&quot;gan1&quot; /&gt;
첫 번째 term은 $q_{\phi}(z|x)$가 prior $p_{\theta}(z)$와 너무 달라지지 않게&lt;/p&gt;

&lt;p&gt;조절하는 regularization term이고 두 번째 term은 latent factor $z$로 부터 원래의 데이터&lt;/p&gt;

&lt;p&gt;$x$를 복원하는데서 생기는 loss에 해당한다. 사실 두 번째 term은 회귀분석의 식과&lt;/p&gt;

&lt;p&gt;정확히 일치한다. &lt;/p&gt;

&lt;p&gt;$y = X\beta + \epsilon$라는 가정에서 출발한다. 이때 입실론은 평균이 0, 표준편차가 $\sigma$인&lt;/p&gt;

&lt;p&gt;정규분포를 따른다. 우리는 $x_{i}^{T}\beta$와 $y_{i}$의 오차를 줄여주고 싶기 때문에&lt;/p&gt;

&lt;p&gt;최적화된 파라미터는 다음 수식을 만족한다.&lt;/p&gt;

&lt;p&gt;$\hat{\beta} = argmin_{\beta} {(y-X\beta)^{2}}$&lt;/p&gt;

&lt;p&gt;$\epsilon$이 정규분포를 따르기 때문에 $y$도 정규분포를 따르게 된다. &lt;/p&gt;

&lt;p&gt;이때 Maximum Likelihood 방식으로 $\beta$를&lt;/p&gt;

&lt;p&gt;구하게 되면 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmWYVM%2FbtqNXcSuqdE%2FswLTtUUB9N07G2T5XguLTk%2Fimg.png&quot; alt=&quot;gan2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국 LSE로 구한 방법과 MLE로 구한 방법이 같아지고, 이는 앞에서 본 VAE의 손실 함수에서 &lt;/p&gt;

&lt;p&gt;두 번째 term에 해당한다. L2 loss를 줄이기 위해서 linear regression은 target의&lt;/p&gt;

&lt;p&gt;평균으로 예측하는 경향이 있기 때문에 VAE로 생성된 함수는 흐려지는 것이다.&lt;/p&gt;

&lt;p&gt;그에 비해 GAN으로 생성하는 이미지는 굉장히 sharp하다. 그럼 이제 수식을 하나하나&lt;/p&gt;

&lt;p&gt;알아보도록 하겠다.&lt;/p&gt;

&lt;h4 id=&quot;objective-function&quot;&gt;&lt;strong&gt;0. Objective Function&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbntm3f%2FbtqNWf90y5I%2FOvcqH8TKxXhE6iRkZUqcx1%2Fimg.png&quot; alt=&quot;gan3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GAN을 학습하는 과정은 결국 이 minimax 문제를 푸는 것이라 할 수 있다.&lt;/p&gt;

&lt;p&gt;D 입장에서는 진짜 데이터일때(첫 번째 term) 진짜라고 예측할 확률을 극대화(1)하고,&lt;/p&gt;

&lt;p&gt;G가 생성한 가짜 데이터일때(두 번째 term) 진짜라고 예측할 확률을 극소화(0)하여&lt;/p&gt;

&lt;p&gt;objective function 값이 커져야 한다.&lt;/p&gt;

&lt;p&gt;G 입장에서는 D가 생성한 데이터를 진짜로 판별할 확률 $D(G(z))$을 극대화하여&lt;/p&gt;

&lt;p&gt;$log(1-D(G(z)))$를 극소화해야하기 때문에 objective function 값이 작아져야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBm36b%2FbtqNSz2GHJf%2FSvrhWlVQ2ceHznrQyH7sBK%2Fimg.png&quot; alt=&quot;gan4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 이렇게 gradient descent/ascent 방법을 적용하면 문제가 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcLsGsH%2FbtqNU1kj5yP%2Fo6AKAZXSB2TgaPmIO7yS3K%2Fimg.png&quot; alt=&quot;gan5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그래프를 보면, G가 제대로 못하는 상황에서 gradient가 작고 잘하는 상황에서&lt;/p&gt;

&lt;p&gt;gradient가 크기 때문에 학습이 원활히 이루어지지 않음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;따라서 실제로 학습을 할 때는&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F54zAz%2FbtqNSzIpyr0%2FUGgURhuNBnLmBtLRobUVP0%2Fimg.png&quot; alt=&quot;gan6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런 식으로 바꾼 다음 gradient를 적용한다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAoBWA%2FbtqNQ3pHTln%2FdWKNWEgJW2z7kfAxqVlCU1%2Fimg.png&quot; alt=&quot;gan7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(a)에서는 generator가 제대로 mapping을 못하고 있고, D도 given G에 대해서 optimal하지 않다.&lt;/p&gt;

&lt;p&gt;(b) D goes to optimum for given G&lt;/p&gt;

&lt;p&gt;(c) G converges to data distribution&lt;/p&gt;

&lt;p&gt;(d) global optimum에 도달. G는 데이터의 분포를 완벽히 학습했고 D는 그냥 찍기가&lt;/p&gt;

&lt;p&gt;되어버렸다.&lt;/p&gt;

&lt;p&gt;그렇다면 실제로 이렇게 학습이 되는지 수식적으로 살펴보도록 하겠다.&lt;/p&gt;

&lt;h4 id=&quot;global-optimality&quot;&gt;&lt;strong&gt;1. Global Optimality&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaLAsU%2FbtqNXFfQ6ZY%2FXkzGoxm7OfaztMhnMUVoc1%2Fimg.png&quot; alt=&quot;gan8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;먼저, G가 주어졌을 때 D의 optimum은 (2)와 같다는 주장이다. LaTex로 옮기기&lt;/p&gt;

&lt;p&gt;귀찮아서.. 내가 쓴 풀이과정은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaLAsU%2FbtqNXFfQ6ZY%2FXkzGoxm7OfaztMhnMUVoc1%2Fimg.png&quot; alt=&quot;gan9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사실 인테그랄을 빼고 뭐 합치고 이런 과정에서 뇌피셜이 꽤나 들어가서.. 정확한 풀이인지는&lt;/p&gt;

&lt;p&gt;모르겠지만 대략 위처럼 구할 수 있다.&lt;/p&gt;

&lt;p&gt;그러면 이제 optimal D에 도달했다고 하고 G에 대해서 최적화를 해보자.&lt;/p&gt;

&lt;p&gt;위의 식을 $C(G)$로 바꾸고, $C(G)$를 minimize하는 G가 존재하는지,&lt;/p&gt;

&lt;p&gt;그렇다면 그 G는 어떤 G인지 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdui0ug%2FbtqNXcLMxOt%2F2sF6Co6Mofs27H7H09M9rK%2Fimg.png&quot; alt=&quot;gan10&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 식을 하나하나 풀면 아래와 같다. 사실은 그냥 $log(4)$를 더하고 뺀 트릭일 뿐…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlHfWy%2FbtqNVSAsVw4%2FLvrrghgHv18siuc7heOm5K%2Fimg.png&quot; alt=&quot;gan11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국, $p_{g} = p_{d}$를 만족하는 G가 optimal G가 되고 이때 optimum value는&lt;/p&gt;

&lt;p&gt;$-log(4)$에 해당한다.&lt;/p&gt;

&lt;p&gt;최적의 값이 존재한다는 것과 알고리즘으로 값을 찾을 수 있다는 다른 문제기 때문에,&lt;/p&gt;

&lt;p&gt;이제 algorithm에 대한 convergence를 증명해야 한다. 이 문제는 매우 간단하게 증명된다.&lt;/p&gt;

&lt;h4 id=&quot;convergence-of-algorithm&quot;&gt;&lt;strong&gt;2. Convergence of Algorithm&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FciHjBM%2FbtqNWhtdOnC%2F6j5zq54L0WsdauSCAHRmm0%2Fimg.png&quot; alt=&quot;gan12&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 식을 $p_{g}$에 대해서 미분하면 $log(1-D_{G}^{*}(x))$만 남기 때문에 $p_{g}$에 대해서&lt;/p&gt;

&lt;p&gt;convex하고, gradient descent에 의해 $p_{g}$는 $p_{data}$로 수렴한다.&lt;/p&gt;

&lt;p&gt;지금까지 GAN의 이론적 근거에 대해서 알아보았다.&lt;/p&gt;

&lt;p&gt;요약하자면&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;1) given G에 대해서 D의 optimum D* 존재, D가 수렴(1차 미분으로 풀 수 있음)&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;2) D*에 대해서 G가 수렴: $p_{g}$가 $p_{d}$를 만족할 때 까지..&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;3) repeat 1), 2) until converged&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음은 GAN으로 생성된 이미지들이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FsSTDc%2FbtqNVSf9gDE%2FuEeuJkhCrrxKhzSR0F8cj1%2Fimg.png&quot; alt=&quot;gan13&quot; /&gt;&lt;/p&gt;

&lt;p&gt;중간중간 부자연스러운 것들도 보이지만, 대체로 학습이 잘되었다.&lt;/p&gt;

&lt;p&gt;그렇다면 GAN은 항상 완벽한 generation을 할 수 있을까? 그건 아니다.&lt;/p&gt;

&lt;p&gt;대표적으로 다음과 같은 문제들이 존재한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;minimax 학습으로 인한 학습의 불안정성, mode collapsing&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;함수 공간을 neural network로 정의하면 함수 공간이 아닌 parameter space가 되기 때문에 가정이 깨짐&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;첫 번째에 대해서 알아보자면, neural network 입장에서는 이게 minimax인지 maxmini인지&lt;/p&gt;

&lt;p&gt;알 길이 없다. 따라서 D가 최적화되지 않은 상태에서 G를 최적화하게 되면&lt;/p&gt;

&lt;p&gt;G는 D가 제일 헷갈리는 샘플만 내놓으면 된다. 또한 data distribution이 여러 개의&lt;/p&gt;

&lt;p&gt;mode가 있을 때(예를 들어, MNIST에서는 숫자 1-10이 이에 해당한다)&lt;/p&gt;

&lt;p&gt;G 입장에서는 1만 완벽하게 생성해도 D가 헷갈리기 때문에 학습이 완료된다.&lt;/p&gt;

&lt;p&gt;(물론 이때 D는 다른 숫자들에 대해서 판별능력이 구리다)&lt;/p&gt;

&lt;p&gt;사실 GAN의 불안정한 학습을 해결하기 위해 여러 방법들이 고안되었고,&lt;/p&gt;

&lt;p&gt;지금도 이에 대해서 활발히 연구가 진행되고 있다고 한다.&lt;/p&gt;

&lt;p&gt;given G에 대해서 D를 optimize하는 것이 만만치 않기 때문에 그렇다.&lt;/p&gt;

&lt;p&gt;두 번째는 보다 일반적인 얘기이다. 위의 모든 증명들은 함수 공간에서 성립하는&lt;/p&gt;

&lt;p&gt;증명인데, 사실 우리는 FC layer, Convolution layer 등 네트워크를 설계하고&lt;/p&gt;

&lt;p&gt;parameter를 최적화한다. 물론 인공신경망이 universal function approximator라는 것은&lt;/p&gt;

&lt;p&gt;잘 알려져 있지만, 우리가 설계한 네트워크가 $f$에 맞을지는 모르는 일..&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;references: &lt;a href=&quot;https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf&quot;&gt;GAN paper,&lt;/a&gt; &lt;a href=&quot;https://jaejunyoo.blogspot.com/2019/05/part-i.html&quot;&gt;jaejunyoo.blogspot.com/2019/05/part-i.html&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Dec 2020 00:00:00 +0900</pubDate>
        <link>http://subong0508.github.io/generative-models/2020/12/19/GAN.html</link>
        <guid isPermaLink="true">http://subong0508.github.io/generative-models/2020/12/19/GAN.html</guid>
        
        <category>GAN</category>
        
        <category>deep-learning</category>
        
        <category>generative-model</category>
        
        
        <category>generative-models</category>
        
      </item>
    
  </channel>
</rss>
