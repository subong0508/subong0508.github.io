<!DOCTYPE html>
<html>

  <head>
  
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    
  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>GAN 리뷰</title>
  <meta name="description" content="오늘 리뷰할 논문은 GAN으로 더 잘 알려진, 적대적 인공신경망을 처음으로 제안한 논문이다.">
  
  <meta name="author" content="Jung Jaeeun">
  <meta name="copyright" content="&copy; Jung Jaeeun 2020">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/monokai_sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="오늘 리뷰할 논문은 GAN으로 더 잘 알려진, 적대적 인공신경망을 처음으로 제안한 논문이다." />
  <meta property="og:url" content="http://subong.github.io/" />
  <meta property="og:site_name" content="Data Vision" />
  <meta property="og:title" content="GAN 리뷰" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://subong.github.io//assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="GAN 리뷰">
  <meta name="twitter:description" content="오늘 리뷰할 논문은 GAN으로 더 잘 알려진, 적대적 인공신경망을 처음으로 제안한 논문이다.">
  <meta name="twitter:image" content="http://subong.github.io//assets/logo.png">
  <meta name="twitter:url" content="http://subong.github.io/">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://subong.github.io//review/2020/12/19/GAN.html">
  <link rel="alternate" type="application/rss+xml" title="Data Vision" href="http://subong.github.io//feed.xml" />
</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Data Vision">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">GAN 리뷰</h1>
      <p class="info">by <strong>Jung Jaeeun</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">December 19, 2020</div>
  <div class="post-categories">
  in 
    
    <a href="/category/review">Review</a>
    
  
  </div>
</section>  

<article class="post-content">
  <p>오늘 리뷰할 논문은 GAN으로 더 잘 알려진, 적대적 인공신경망을 처음으로 제안한 논문이다.</p>

<p>뭐 이미 말할 필요도 없이 유명하기도 하고, 수많은 variation이 나오기도 했다.</p>

<p>논문의 제 1저자인 이안 굿펠로우는 GAN의 <strong>generator를 지폐위조범으로, discriminator를</strong></p>

<p><strong>경찰로 묘사했다.</strong> 지폐위조범은 위조 지폐를 더욱 더 진짜 같이 위조하고, 경찰은 그걸 구분하려고</p>

<p>경쟁하는 과정에서 상호발전이 일어난다는 것이다. 사실 우리의 목표는 경찰보다는</p>

<p>지폐위조범의 생성능력을 상승시키는 것에 있다. 결국 학습의 끝에서는 경찰이</p>

<p>어느 것이 위조 지폐인지 진짜 지폐인지 알아볼 수 없도록 말이다.</p>

<p>이렇듯 기본적인 아이디어는 굉장히 직관적이지만, 이론적 배경은 꽤나 탄탄하다.</p>

<p>이제 차근차근 하나씩 알아보겠다.</p>

<p><br /></p>

<hr />

<p><br /></p>

<p>GAN을 알아보기 전, VAE에 대해서 먼저 잠깐 짚고 넘어가자.</p>

<p>VAE의 가장 큰 문제점은 blurry한 이미지가 생성된다는 것이다. </p>

<p>여러가지 설명이 있을 수 있겠지만, 필자가 생각하기로는 VAE의 loss function에서</p>

<p>기인한다고 생각한다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdC2FQH%2FbtqNQ4vjwsv%2F1j5oUsSiBtmeek9MJyFld0%2Fimg.png" alt="gan1" />
첫 번째 term은 $q_{\phi}(z|x)$가 prior $p_{\theta}(z)$와 너무 달라지지 않게</p>

<p>조절하는 regularization term이고 두 번째 term은 latent factor $z$로 부터 원래의 데이터</p>

<p>$x$를 복원하는데서 생기는 loss에 해당한다. 사실 두 번째 term은 회귀분석의 식과</p>

<p>정확히 일치한다. </p>

<p>$y = X\beta + \epsilon$라는 가정에서 출발한다. 이때 입실론은 평균이 0, 표준편차가 $\sigma$인</p>

<p>정규분포를 따른다. 우리는 $x_{i}^{T}\beta$와 $y_{i}$의 오차를 줄여주고 싶기 때문에</p>

<p>최적화된 파라미터는 다음 수식을 만족한다.</p>

<p>$\hat{\beta} = argmin_{\beta} {(y-X\beta)^{2}}$</p>

<p>$\epsilon$이 정규분포를 따르기 때문에 $y$도 정규분포를 따르게 된다. </p>

<p>이때 Maximum Likelihood 방식으로 $\beta$를</p>

<p>구하게 되면 다음과 같다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmWYVM%2FbtqNXcSuqdE%2FswLTtUUB9N07G2T5XguLTk%2Fimg.png" alt="gan2" /></p>

<p>결국 LSE로 구한 방법과 MLE로 구한 방법이 같아지고, 이는 앞에서 본 VAE의 손실 함수에서 </p>

<p>두 번째 term에 해당한다. L2 loss를 줄이기 위해서 linear regression은 target의</p>

<p>평균으로 예측하는 경향이 있기 때문에 VAE로 생성된 함수는 흐려지는 것이다.</p>

<p>그에 비해 GAN으로 생성하는 이미지는 굉장히 sharp하다. 그럼 이제 수식을 하나하나</p>

<p>알아보도록 하겠다.</p>

<h4 id="objective-function"><strong>0. Objective Function</strong></h4>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbntm3f%2FbtqNWf90y5I%2FOvcqH8TKxXhE6iRkZUqcx1%2Fimg.png" alt="gan3" /></p>

<p>GAN을 학습하는 과정은 결국 이 minimax 문제를 푸는 것이라 할 수 있다.</p>

<p>D 입장에서는 진짜 데이터일때(첫 번째 term) 진짜라고 예측할 확률을 극대화(1)하고,</p>

<p>G가 생성한 가짜 데이터일때(두 번째 term) 진짜라고 예측할 확률을 극소화(0)하여</p>

<p>objective function 값이 커져야 한다.</p>

<p>G 입장에서는 D가 생성한 데이터를 진짜로 판별할 확률 $D(G(z))$을 극대화하여</p>

<p>$log(1-D(G(z)))$를 극소화해야하기 때문에 objective function 값이 작아져야 한다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FBm36b%2FbtqNSz2GHJf%2FSvrhWlVQ2ceHznrQyH7sBK%2Fimg.png" alt="gan4" /></p>

<p>하지만 이렇게 gradient descent/ascent 방법을 적용하면 문제가 있다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcLsGsH%2FbtqNU1kj5yP%2Fo6AKAZXSB2TgaPmIO7yS3K%2Fimg.png" alt="gan5" /></p>

<p>위의 그래프를 보면, G가 제대로 못하는 상황에서 gradient가 작고 잘하는 상황에서</p>

<p>gradient가 크기 때문에 학습이 원활히 이루어지지 않음을 알 수 있다.</p>

<p>따라서 실제로 학습을 할 때는</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F54zAz%2FbtqNSzIpyr0%2FUGgURhuNBnLmBtLRobUVP0%2Fimg.png" alt="gan6" /></p>

<p>이런 식으로 바꾼 다음 gradient를 적용한다고 한다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FAoBWA%2FbtqNQ3pHTln%2FdWKNWEgJW2z7kfAxqVlCU1%2Fimg.png" alt="gan7" /></p>

<p>(a)에서는 generator가 제대로 mapping을 못하고 있고, D도 given G에 대해서 optimal하지 않다.</p>

<p>(b) D goes to optimum for given G</p>

<p>(c) G converges to data distribution</p>

<p>(d) global optimum에 도달. G는 데이터의 분포를 완벽히 학습했고 D는 그냥 찍기가</p>

<p>되어버렸다.</p>

<p>그렇다면 실제로 이렇게 학습이 되는지 수식적으로 살펴보도록 하겠다.</p>

<h4 id="global-optimality"><strong>1. Global Optimality</strong></h4>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaLAsU%2FbtqNXFfQ6ZY%2FXkzGoxm7OfaztMhnMUVoc1%2Fimg.png" alt="gan8" /></p>

<p>먼저, G가 주어졌을 때 D의 optimum은 (2)와 같다는 주장이다. LaTex로 옮기기</p>

<p>귀찮아서.. 내가 쓴 풀이과정은 아래와 같다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaLAsU%2FbtqNXFfQ6ZY%2FXkzGoxm7OfaztMhnMUVoc1%2Fimg.png" alt="gan9" /></p>

<p>사실 인테그랄을 빼고 뭐 합치고 이런 과정에서 뇌피셜이 꽤나 들어가서.. 정확한 풀이인지는</p>

<p>모르겠지만 대략 위처럼 구할 수 있다.</p>

<p>그러면 이제 optimal D에 도달했다고 하고 G에 대해서 최적화를 해보자.</p>

<p>위의 식을 $C(G)$로 바꾸고, $C(G)$를 minimize하는 G가 존재하는지,</p>

<p>그렇다면 그 G는 어떤 G인지 알아보자.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdui0ug%2FbtqNXcLMxOt%2F2sF6Co6Mofs27H7H09M9rK%2Fimg.png" alt="gan10" /></p>

<p>위의 식을 하나하나 풀면 아래와 같다. 사실은 그냥 $log(4)$를 더하고 뺀 트릭일 뿐…</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlHfWy%2FbtqNVSAsVw4%2FLvrrghgHv18siuc7heOm5K%2Fimg.png" alt="gan11" /></p>

<p>결국, $p_{g} = p_{d}$를 만족하는 G가 optimal G가 되고 이때 optimum value는</p>

<p>$-log(4)$에 해당한다.</p>

<p>최적의 값이 존재한다는 것과 알고리즘으로 값을 찾을 수 있다는 다른 문제기 때문에,</p>

<p>이제 algorithm에 대한 convergence를 증명해야 한다. 이 문제는 매우 간단하게 증명된다.</p>

<h4 id="convergence-of-algorithm"><strong>2. Convergence of Algorithm</strong></h4>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FciHjBM%2FbtqNWhtdOnC%2F6j5zq54L0WsdauSCAHRmm0%2Fimg.png" alt="gan12" /></p>

<p>위의 식을 $p_{g}$에 대해서 미분하면 $log(1-D_{G}^{*}(x))$만 남기 때문에 $p_{g}$에 대해서</p>

<p>convex하고, gradient descent에 의해 $p_{g}$는 $p_{data}$로 수렴한다.</p>

<p>지금까지 GAN의 이론적 근거에 대해서 알아보았다.</p>

<p>요약하자면</p>

<ul>
  <li>
    <p><strong>1) given G에 대해서 D의 optimum D* 존재, D가 수렴(1차 미분으로 풀 수 있음)</strong></p>
  </li>
  <li>
    <p><strong>2) D*에 대해서 G가 수렴: $p_{g}$가 $p_{d}$를 만족할 때 까지..</strong></p>
  </li>
  <li>
    <p><strong>3) repeat 1), 2) until converged</strong></p>
  </li>
</ul>

<p>다음은 GAN으로 생성된 이미지들이다.</p>

<p><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FsSTDc%2FbtqNVSf9gDE%2FuEeuJkhCrrxKhzSR0F8cj1%2Fimg.png" alt="gan13" /></p>

<p>중간중간 부자연스러운 것들도 보이지만, 대체로 학습이 잘되었다.</p>

<p>그렇다면 GAN은 항상 완벽한 generation을 할 수 있을까? 그건 아니다.</p>

<p>대표적으로 다음과 같은 문제들이 존재한다.</p>

<ol>
  <li>
    <p>minimax 학습으로 인한 학습의 불안정성, mode collapsing</p>
  </li>
  <li>
    <p>함수 공간을 neural network로 정의하면 함수 공간이 아닌 parameter space가 되기 때문에 가정이 깨짐</p>
  </li>
</ol>

<p>첫 번째에 대해서 알아보자면, neural network 입장에서는 이게 minimax인지 maxmini인지</p>

<p>알 길이 없다. 따라서 D가 최적화되지 않은 상태에서 G를 최적화하게 되면</p>

<p>G는 D가 제일 헷갈리는 샘플만 내놓으면 된다. 또한 data distribution이 여러 개의</p>

<p>mode가 있을 때(예를 들어, MNIST에서는 숫자 1-10이 이에 해당한다)</p>

<p>G 입장에서는 1만 완벽하게 생성해도 D가 헷갈리기 때문에 학습이 완료된다.</p>

<p>(물론 이때 D는 다른 숫자들에 대해서 판별능력이 구리다)</p>

<p>사실 GAN의 불안정한 학습을 해결하기 위해 여러 방법들이 고안되었고,</p>

<p>지금도 이에 대해서 활발히 연구가 진행되고 있다고 한다.</p>

<p>given G에 대해서 D를 optimize하는 것이 만만치 않기 때문에 그렇다.</p>

<p>두 번째는 보다 일반적인 얘기이다. 위의 모든 증명들은 함수 공간에서 성립하는</p>

<p>증명인데, 사실 우리는 FC layer, Convolution layer 등 네트워크를 설계하고</p>

<p>parameter를 최적화한다. 물론 인공신경망이 universal function approximator라는 것은</p>

<p>잘 알려져 있지만, 우리가 설계한 네트워크가 $f$에 맞을지는 모르는 일..</p>

<hr />

<p>references: <a href="https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">GAN paper,</a> <a href="https://jaejunyoo.blogspot.com/2019/05/part-i.html">jaejunyoo.blogspot.com/2019/05/part-i.html</a></p>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/GAN">GAN</a>,&nbsp;<a href="/tag/deeplearning">deeplearning</a>,&nbsp;<a href="/tag/generativemodel">generativemodel</a>
</section>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
  
</section>




</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Data Vision</h3>

    <div class="site-navigation">
      
      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:subong0508@naver.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">subong0508@naver.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/subong0508" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">subong0508</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My personal blog for studying AI
</p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script>

$(document).ready(function() {

  // Syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });


});

</script>




  </body>

</html>
