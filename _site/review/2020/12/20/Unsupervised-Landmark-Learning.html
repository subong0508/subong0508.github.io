<!DOCTYPE html>
<html>

  <head>
  
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    
  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Unsupervised Landmark Learning</title>
  <meta name="description" content="*1. Unsupervised Learning of Object Landmarks through Conditional Image Generation">
  
  <meta name="author" content="Jung Jaeeun">
  <meta name="copyright" content="&copy; Jung Jaeeun 2021">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/styles/monokai_sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="*1. Unsupervised Learning of Object Landmarks through Conditional Image Generation" />
  <meta property="og:url" content="http://subong0508.github.io" />
  <meta property="og:site_name" content="Data Vision" />
  <meta property="og:title" content="Unsupervised Landmark Learning" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://subong0508.github.io/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Unsupervised Landmark Learning">
  <meta name="twitter:description" content="*1. Unsupervised Learning of Object Landmarks through Conditional Image Generation">
  <meta name="twitter:image" content="http://subong0508.github.io/assets/logo.png">
  <meta name="twitter:url" content="http://subong0508.github.io">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://subong0508.github.io/review/2020/12/20/Unsupervised-Landmark-Learning.html">
  <link rel="alternate" type="application/rss+xml" title="Data Vision" href="http://subong0508.github.io/feed.xml" />
</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/logo.png" alt="Data Vision">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
        
          
          <li class="nav-link"><a href="/about/">About</a>
          
        
          
        
          
        
          
        
          
          <li class="nav-link"><a href="/posts/">Posts</a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Unsupervised Landmark Learning</h1>
      <p class="info">by <strong>Jung Jaeeun</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">December 20, 2020</div>
  <div class="post-categories">
  in 
    
    <a href="/category/review">Review</a>
    
  
  </div>
</section>  

<article class="post-content">
  <h1 id="unsupervised-learning-of-object-landmarks-through-conditional-image-generationhttpspapersnipsccpaper7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generationpdf"><a href="https://papers.nips.cc/paper/7657-unsupervised-learning-of-object-landmarks-through-conditional-image-generation.pdf">*1. Unsupervised Learning of Object Landmarks through Conditional Image Generation</a></h1>

<h2 id="task-generate-the-target-image-given-the-source-image-and-the-encoded-target-image">Task: Generate the target image given the source image and the encoded target image.</h2>

<h2 id="method">Method</h2>
<h3 id="heatmaps-bottleneck">Heatmaps bottleneck</h3>
<p>$\Phi(x)$: learn to extract keypoint-like structures</p>

<p>$S_{u}(x;k)$: K heatmaps
=&gt; $u_{k}^{*}(x) = \frac{\sum ue^{S_{u}(x;k)}}{\sum e^{S_{u}(x;k)}}$ (spatial softmax)</p>

<p>$\Phi_{u}(x;k)$: Gaussian-like function centered at $u_{k}^{*}$</p>

<h3 id="generator-perceptual-loss">Generator: Perceptual Loss</h3>
<p>$\mathcal{L}(x’, \hat{x’}) = \sum_{l} \alpha_{l} \vert\vert\Gamma_{l}(x’)-\Gamma_{l}(\hat{x’})\vert\vert^{2}$: perceptual loss</p>

<h2 id="overall-architecture">Overall Architecture</h2>
<p><img src="../../../../img/Unsupervised-Landmark/cond.png" alt="model architecture" /></p>

<h2 id="summary">Summary</h2>
<p><strong>By using K-heatmaps as a bottleneck having keypoints information and exploiting training method of conditional image generation, this paper achieved SOTA results of landmark detection</strong></p>

<h1 id="unsupervised-landmark-learning-from-unpaired-datahttpsarxivorgpdf200701053pdf"><a href="https://arxiv.org/pdf/2007.01053.pdf">*2. Unsupervised Landmark Learning from unpaired data</a></h1>

<h2 id="task-reconstructing-images-with-the-apprearance-and-pose-originated-from-different-images-and-establish-various-consistencies-among-these-reconstructed-images">Task: Reconstructing images with the apprearance and pose originated from different images and establish various consistencies among these reconstructed images</h2>

<h2 id="method-1">Method</h2>
<h3 id="cross-image-cycle-consistency-framework">Cross-Image cycle Consistency Framework</h3>
<p>Given $I_{i}, I_{j}$: pair of images</p>

<p>$\mathbf {a_{i}} = E_{a}(N(I_{i})), \mathbf {a_{j}} = E_{a}(N(I_{j}))$</p>

<p>$\mathbf {p_{i}} = E_{p}(N(I_{i})), \mathbf {p_{j}} = E_{p}(N(I_{j}))$</p>

<p>$I_{i, j} = D(\mathbf {a_{i}}, \mathbf {p_{j}}), I_{j, i} = D(\mathbf {a_{j}}, \mathbf {p_{i}})$</p>

<p>$\mathbf {a_{i}}’ = E_{a}(N(I_{i, j})), \mathbf {a_{j}}’ = E_{a}(N(I_{j, i}))$</p>

<p>$\mathbf {p_{i}}’ = E_{p}(N(I_{i,j})), \mathbf {p_{j}}’ = E_{p}(N(I_{j,i}))$</p>

<p>$I_{i, j}’ = D(\mathbf {a_{i}}’, \mathbf {p_{j}}’), I_{j, i}’ = D(\mathbf {a_{j}}’, \mathbf {p_{i}}’)$</p>

<p><strong>$\mathcal{L_{cycle}} = \mathcal{P}(I_{i}, I_{i}’)+\mathcal{P}(I_{j}, I_{j}’)$: P is perceptual loss implemented by VGG network</strong></p>

<p><strong>$\mathcal{L_{inv}} = \vert\vert\mathbf {p_{i}}’ - \mathbf{p_{i}}\vert\vert ^ {2} + \vert\vert\mathbf {p_{j}}’ - \mathbf{p_{j}}\vert\vert ^ {2}$</strong></p>

<h3 id="regularization-via-cross-image-flow-module">Regularization via Cross-Image Flow Module</h3>

<p>$T^{i \rightarrow j}$: the location correspondences</p>

<p>$\mathbf{C}$: 4D tensor containing the element-wise cosine similiarity between two feature maps($\mathbf {f_{i}}’, \mathbf {f_{j}}’$)</p>

<p>$\hat{C} = \mathbf {W_{1}} \bigotimes (\mathbf {W_{2}} \bigotimes \mathbf {C})$: 4D convolution using 2D convolutions consequently</p>

<p>$S^{i \rightarrow j}(x_{j}, y_{j}) = softmax(\hat{\mathbf{C}}(*, *, x_{j}, y_{j}))$</p>

<p>$T^{i \rightarrow j}(x_{j}, y_{j}) = argmax_{(x_{i}, y_{i})}S^{i \rightarrow j}(x_{j}, y_{j})$ (vice versa for j-&gt;i)</p>

<p>Thus, transformation maps can reflect the semantic correlations between landmarks of two images.</p>

<p><strong>$\mathcal{L_{equiv}} = \vert\vert\mathbf {p_{i}} - T^{j \rightarrow i}\circ\mathbf{p_{j}}\vert\vert ^ {2}+\vert\vert\mathbf {p_{j}} - T^{i \rightarrow j}\circ\mathbf{p_{i}}\vert\vert ^ {2}$</strong></p>

<h3 id="final-loss-mathcalltotallambdacyclemathcallcyclelambdaequivmathcallequivlambdainvmathcallinv">Final Loss: $\mathcal{L_{total}}=\lambda_{cycle}\mathcal{L_{cycle}}+\lambda_{equiv}\mathcal{L_{equiv}}+\lambda_{inv}\mathcal{L_{inv}}$</h3>

<h2 id="overall-architecture-1">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/unpaired.png" alt="model architecture" /></p>

<h2 id="summary-1">Summary</h2>
<p><strong>Through techinique of balancing losses which can be categorized into meaningful embedding/invariance/equivariance, unpaired images can be used to locate landmarks</strong></p>

<h1 id="unsupervised-part-based-disentangling-of-object-shape-and-appearancehttpsarxivorgpdf190306946pdf"><a href="https://arxiv.org/pdf/1903.06946.pdf">*3. Unsupervised Part-Based Disentangling of Object Shape and Appearance</a></h1>

<h2 id="method-2">Method</h2>

<h3 id="part-based-representation">Part-based Representation</h3>

<ul>
  <li>part based factorization of representation: $\phi(x) := (\phi_{1}(x), \phi_{2}(x), …)^{\top}$ where $\phi_{i}(x)$ can be decomposed as $[\alpha_{i}(x), \sigma_{i}(x)]$</li>
</ul>

<h3 id="invariance-and-equivariance">Invariance and Equivariance</h3>

<ul>
  <li>invariance: $i)\alpha_{i}(x \circ s)=\alpha_{i}(x)$, $ii)\sigma_{i}(a(x))=\sigma_{i}(x)$ =&gt; <strong>$\mathcal {L_{rec}} = \vert\vert x - D([\alpha_{i=1,…}(x), \sigma_{i=1,…}(x)]) \vert\vert_{1}$</strong></li>
  <li>equivariance: $\sigma_{i}(x \circ s) = \sigma_{i}(x) \circ s$ =&gt; <strong>$\mathcal {L_{equiv}} = \sum_{i} \lambda_{\mu} \vert\vert \mu[\sigma_{i}(x \circ s)] - \mu[\sigma_{i}(a(x))\circ s]\vert\vert_{2} + \lambda_{\sum} \vert\vert \sum[\sigma_{i}(x \circ s)] - \sum[\sigma_{i}(a(x))\circ s]\vert\vert_{1}$</strong></li>
  <li>total loss: $\mathcal {L} = \mathcal {L_{rec}} + \mathcal {L_{equiv}}$</li>
</ul>

<h2 id="overall-architecture-2">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/shape-appear.png" alt="model architecture" /></p>

<p>where $f$ denotes localized image encoding</p>

<h2 id="summary-2">Summary</h2>
<p><strong>To sum up, shape stream extracts part shapes which are independent from appreances and appreance stream does same thing as well except it re-encodes part appearances using local features. In decoder, reconstruction is done by using approximate part shapes(normalized) and part appreances weighted on part shapes</strong></p>

<h1 id="self-supervised-learning-of-interpretable-keypoints-from-unlabelled-videoshttpswwwrobotsoxacukvggpublications2020jakab20jakab20pdf"><a href="https://www.robots.ox.ac.uk/~vgg/publications/2020/Jakab20/jakab20.pdf">*4. Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos</a></h1>

<h2 id="task-recognizing-the-pose-of-objects-from-a-single-image-that-for-learning-uses-only-unlabelled-videos-and-a-weak-empirical-prior-on-the-objects-poses">Task: Recognizing the pose of objects from a single image that for learning uses only unlabelled videos and a weak empirical prior on the objects poses</h2>

<h2 id="method-3">Method</h2>

<ul>
  <li>${y} = \Phi({x})$: pose extracted from image</li>
  <li>$\Psi(\Phi(x), x’)$: conditional decoder network</li>
</ul>

<h3 id="dual-representation-of-pose--bottleneck">Dual representation of pose &amp; bottleneck</h3>

<ul>
  <li>$\mathbf{p} = (p_{1}, …, p_{K})$: a vector of K 2D keypoint coordinates trained by $\eta(\mathbf {y})$</li>
  <li>$y* = \beta(\mathbf{p})_{u}$: a distance field from line segments that forms the skeleton image</li>
  <li>$x=\Psi(\beta \circ \eta \circ \Phi(x), x’)$: so as to prevent cheating by exploting dual representation</li>
</ul>

<h3 id="loss">Loss</h3>
<ul>
  <li>$\mathcal{L_{perc}}=\frac{1}{N}\sum_{i=1}^{N}{\vert\vert \Gamma(\hat{x_{i}}) - \Gamma(x_{i}) \vert\vert ^ {2}}$: <strong>Auto-encoding</strong> loss implemented by perceptual loss</li>
  <li>$\mathcal{L_{disc}}(D) = \frac{1}{M}\sum_{j=1}^{M} D(\bar{y_{j}})^{2} + \frac{1}{N}\sum_{i=1}^{N}{(1-D(y_{i}))^{2}}$: <strong>Difference adversarial loss</strong> to match $p(y)\approx q(y)$, encourages the images $y$ ro be ‘skeleton-like’</li>
  <li>$\mathcal{L}(\Phi, \Psi, D) = \lambda_{disc}\mathcal{L_{disc}}(D)+\mathcal{L_{perc}}(\Psi, \Phi)$: <strong>Total Loss</strong>, minimized w.r.t $\Phi, \Psi$ and maximised w.r.t $D$</li>
</ul>

<h2 id="overall-architecture-3">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/pose.png" alt="model architecture" /></p>

<h2 id="summary-3">Summary</h2>
<p><strong>While using conditional image generator as a decoder, the method utilized in this paper forces encoder to grab meaningful pose information by exploiting dual representation and empirical prior</strong></p>

<h1 id="unsupervised-learning-of-landmarks-by-descriptor-vector-exchangehttpsarxivorgpdf190806427pdf"><a href="https://arxiv.org/pdf/1908.06427.pdf">5. Unsupervised Learning of Landmarks by Descriptor Vector Exchange</a></h1>

<h2 id="method-4">Method</h2>

<ul>
  <li>equivariance constraint: $\Phi_{u}(x) = \Phi_{gu}(x) \text{ where } \Phi \text{ correspondes to dense embedding}$</li>
  <li>probabilistic formulation: $p(v \vert u;\Phi, x, x’)= \frac{e^{&lt;\Phi_{u}(x), \Phi_{v}(x)&gt;}}{\int_{\Omega} e^{&lt;\Phi_{u}(x), \Phi_{t}(x)&gt;}dt}$</li>
  <li>$\mathcal {L}(\Phi;x, x’, g) = E_{u, v}[\vert\vert v - gu \vert\vert]$</li>
</ul>

<h3 id="vector-exchangeability">Vector exchangeability</h3>

<ul>
  <li>$x_{\alpha}$: auxiliary image that belongs to same-category in $u, v$</li>
  <li>$\hat {\Phi_{u}(x\vert x_{\alpha})} = \int \Phi_{w}(x_{\alpha})p(w \vert u; \Phi, x, x_{\alpha})dw$</li>
  <li>new probabilistic formulation: $p(v \vert u;\Phi, x, x’)= \frac{e^{&lt;\hat{\Phi_{u}}(x), \Phi_{v}(x)&gt;}}{\int_{\Omega} e^{&lt;\hat{\Phi_{u}}(x), \Phi_{t}(x)&gt;}dt}$</li>
</ul>

<h2 id="overall-architecture-4">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/DVE.png" alt="model architecture" /></p>

<h2 id="summary-4">Summary</h2>
<p><strong>DVE captures dense embedding of an image which acts as an invariant descriptor vector by enforcing it’s robustness via intra-class variants(auxiliary images), in which it learns equivariance and intra-class invariance simultaneously</strong></p>

<h1 id="unsupervised-learning-of-facial-landmarks-based-on-inter-intra-subject-consistencieshttpsarxivorgpdf200407936pdf"><a href="https://arxiv.org/pdf/2004.07936.pdf">6. Unsupervised Learning of Facial Landmarks based on Inter-Intra Subject Consistencies</a></h1>

<h2 id="method-5">Method</h2>
<ol>
  <li><em>Landmark Detector</em>: using visual feature maps $S \in \mathbf{R}^{H \times W \times K}$,<br />
the predicted $k$-th landmark location $u_{k}$ is weighted average of the spatial locations $i$<br />
$\Phi_{H}(x;k) = \exp(-\frac{1}{2\sigma^{2}} \vert\vert u-u_{k} \vert\vert ^ {2})$: Gaussian-like probabilistic heatmap centered at $u_{k}$</li>
  <li><em>Inter-Intra Image Generator</em>
    <ul>
      <li>$x’, x^{a}$: deformed image, auxiliary image</li>
      <li>$\mathcal {F_{s}} = \Phi_{E}(x) \in \mathbf {R^{H \times W \times D}}$: a visual feature map</li>
      <li>$\mathcal {I_{a}} = \Psi(\mathcal {F_{s}}, \Phi_{H}(x^{a})) = \Psi(\Phi_{E}(x), \Phi_{H}(x^{a}))$</li>
      <li>$\mathcal {I} = \Psi(\mathcal {F_{t}}, \Phi_{H}(x’)) = \Psi(\Phi_{E}(\mathcal {I_{a}}), \Phi_{H}(x’))$</li>
    </ul>
  </li>
  <li><em>Cycle Backward Path</em>: both $X$ and $X’$ are reconstructed through it’s counterpart</li>
</ol>

<h3 id="training">Training</h3>
<ul>
  <li>reconstruction loss: $\mathcal {L_{R}}(\mathcal{I}, \mathcal{I_{gt}}) = \vert\vert \mathcal{I}-\mathcal{I_{gt}} \vert\vert ^{2}$</li>
  <li>perceptual loss: $\mathcal {L_{P}}(\mathcal{I}, \mathcal{I_{gt}}) = \sum_{l} \vert\vert VGG^{l}(\mathcal{I}) - VGG^{l}(\mathcal{I_{gt}})\vert\vert ^ {2}$</li>
  <li>total loss: $\mathcal{L} = \mathcal{L_{R}}(\mathcal{I_{x}}, x)+\mathcal{L_{R}}(\mathcal{I_{x’}}, x’)+\mathcal{L_{P}}(\mathcal{I_{x}}, x)+\mathcal{L_{P}}(\mathcal{I_{x’}}, x’)$</li>
</ul>

<h2 id="overall-architecture-5">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/inter-intra.png" alt="model architecture" /></p>

<h2 id="summary-5">Summary</h2>
<p><strong>By inserting auxiliary image’s structure when reconstructing the target image, this method gives the model intra-subject consistency and reinforeces inter-subject consistency via cycle backward path</strong></p>

<h1 id="unsupervised-disentanglement-of-pose-appearance-and-background-from-images-and-videoshttpsarxivorgpdf200109518pdf"><a href="https://arxiv.org/pdf/2001.09518.pdf">7. Unsupervised Disentanglement of Pose, Appearance and Background from Images and Videos</a></h1>

<h1 id="unsupervised-discovery-of-object-landmarks-via-contrastive-learninghttpsarxivorgpdf200614787pdf"><a href="https://arxiv.org/pdf/2006.14787.pdf">*8. Unsupervised Discovery of Object Landmarks via Contrastive Learning</a></h1>

<h2 id="method-6">Method</h2>

<h3 id="contrastive-learning">Contrastive learning</h3>
<p>The goal is to learn: $\langle\Phi(x), \Phi(x’)\rangle \gg \langle\Phi(x), \Phi(z)\rangle$</p>

<h3 id="traditional-methods">Traditional methods</h3>
<ul>
  <li>Equivariant learning: enforces model to be robust to geometric/photometric transformations</li>
  <li>Invariant learning: encourages the representations to be invariant to transformations while being distinctive across images</li>
  <li>Trade off: Equivariant learning makes model to be less deeper(due to pooling operation), while invariant learning makes model to be more deeper(to capture more sophisticated features).</li>
</ul>

<h3 id="novel-approach-incorporating-1-2">Novel approach: incorporating 1, 2</h3>
<ol>
  <li>$\mathcal{L_{NCE}} = -log\frac{\exp{(\langle \Phi(x), \Phi(x’) \rangle)}}{\sum_{i=1}^{N}\exp{(\langle \Phi(x), \Phi(x_{i}) \rangle)}}$</li>
  <li><strong>Hypercolumns</strong>: $\Phi_{u}(x) = \Phi_{u}^{k_{1}}(x) \bigoplus \Phi_{u}^{k_{2}}(x) … \bigoplus \Phi_{u}^{k_{n}}(x)$</li>
</ol>

<h2 id="overall-architecture-6">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/contrastive.png" alt="model architecture" /></p>

<h2 id="summary-6">Summary</h2>
<p><strong>In contrast to traditional approach that can be categorized into equivariant/invariant learning, the method used in this paper incorporates both approachs by using contrastive learning technique. To be specific, it trains invariant representations using constrastive learning and then extracts hypercolumn representation to detect landmarks.</strong></p>

<h1 id="brul-barycenter-regularized-unsupervised-landmark-extractionhttpsarxivorgpdf200611643pdf"><a href="https://arxiv.org/pdf/2006.11643.pdf">9. BRULÉ: Barycenter-Regularized Unsupervised Landmark Extraction</a></h1>

<h1 id="unsupervised-learning-of-object-frames-by-dense-equivariant-image-labellinghttpsarxivorgpdf170602932pdf"><a href="https://arxiv.org/pdf/1706.02932.pdf">*10. Unsupervised learning of object frames by dense equivariant image labelling</a></h1>

<h2 id="method-7">Method</h2>

<ul>
  <li>invariant constraint: $\Phi(\mathbb{x}, u) = \Phi(g\mathbb{x}, gu)$</li>
</ul>

<p>Motivated by invariant constraints, the similarity $\langle \Phi(\mathbb{x}, u), \Phi(\mathbb{x’}, gu)\rangle$ should be larger than the similarity $\langle \Phi(\mathbb{x}, u), \Phi(\mathbb{x’}, v)\rangle$ where $g$ is an arbitrary optical flow module.</p>

<h3 id="loss-1">Loss</h3>
<ul>
  <li>$\mathcal{L_{\log}}(\Phi \vert \mathbb{x}, \mathbb{x’}, g) = -\frac{1}{HW}\sum_{u}\log p(gu \vert u; \mathbb{x}, \mathbb{x’}, \Phi)$</li>
  <li>$\mathcal{L_{\text{dist}}}(\Phi \vert \mathbb{x}, \mathbb{x’}, g) = \frac{1}{HW}\sum_{u}\sum_{v} \vert\vert v-gu \vert\vert_{2}^{\gamma} p(v \vert u; \mathbb{x}, \mathbb{x’}, \Phi)$</li>
</ul>

<h2 id="overall-architecture-7">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/dense-labelling.png" alt="model architecture" /></p>

<h1 id="unsupervised-learning-of-object-landmarks-by-factorized-spatial-embeddingshttpsarxivorgpdf170502193pdf"><a href="https://arxiv.org/pdf/1705.02193.pdf">*11. Unsupervised learning of object landmarks by factorized spatial embeddings</a></h1>

<h2 id="method-8">Method</h2>

<h3 id="deformable-objects-equivariance">Deformable objects: Equivariance</h3>
<p>$\forall r \in S_{0} : \Phi(r;\mathbb{x} \circ g) = g(\Phi(r;\mathbb{x}))$</p>
<h3 id="semantically-consistent-network">Semantically consistent network</h3>
<ul>
  <li>$\Psi(\mathbb{x}) \in \mathbb{R^{H \times W \times K}}$: score maps</li>
  <li>$p(u \vert \mathbb{x}, r) = \frac{e ^ {\Psi(\mathbb{x})}}{\sum_{v} e^{\Psi(\mathbb{x})}}$</li>
  <li>$u_{r}^{*} = \sum_{u} up(u \vert \mathbb{x}, r)$</li>
</ul>

<p>=&gt; $\mathcal{L_{align}} = \frac{1}{K}\sum_{r=1}^{K}\sum_{uv} \vert\vert u-g(v) \vert\vert ^ {2}p(u \vert \mathbb{x}, r)p(v \vert \mathbb{x’}, r)$</p>

<h3 id="diversity-loss-penalize-the-mutual-overlap">Diversity Loss: penalize the mutual overlap</h3>
<ul>
  <li>$\mathcal{L_{div}}(x) = \frac{1}{K^{2}} \sum_{r=1}^{K}\sum_{r’=1}^{K} \sum_{u}p(u \vert \mathbb{x}, r)p(u \vert \mathbb{x}, r’)$</li>
  <li>$\mathcal{L_{div}’}(x)=K - \sum_{u} \max_{r=1,…,K}\sum_{\delta_{u}}p(mu+\delta_{u} \vert \mathbb{x}, r)$: $m \times m$ sum pooling</li>
</ul>

<h3 id="total-loss">Total Loss</h3>
<p>$\mathcal{L_{total}} = \lambda\mathcal{R}(\Psi) + \frac{1}{N} \sum_{i=1}^{N}\mathcal{L_{align}’}(\mathbb{x}, \mathbb{x_{i}}, g_{i};\Psi) + \gamma \mathcal{L_{div}’’}(\mathbb{x_{i}};\Psi) + \gamma \mathcal{L_{div}’’}(\mathbb{x’_{i}};\Psi)$</p>

<h2 id="overall-architecture-8">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/spa.png" alt="model architecture" /></p>

<h2 id="summary-7">Summary</h2>
<p><strong>By factorzing deformations, we can learn intrinsic reference frame for the object</strong></p>

<h1 id="deforming-autoencoders-unsupervised-disentangling-of-shape-and-appearancehttpsarxivorgpdf180606503pdf"><a href="https://arxiv.org/pdf/1806.06503.pdf">12. Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance</a></h1>

<h1 id="self-supervised-learning-of-a-facial-attribute-embedding-from-videohttpsarxivorgpdf180806882pdf"><a href="https://arxiv.org/pdf/1808.06882.pdf">13. Self-supervised learning of a facial attribute embedding from video</a></h1>

<h2 id="method-9">Method</h2>
<h3 id="multi-source-frames-architecture-predicting-a-confidence-heatmap">Multi-source frames architecture: predicting a confidence heatmap</h3>
<h3 id="curriculum-strategy-stop-training-when-samples-fall-into-90th-100th-percentile-range">Curriculum Strategy: stop training when samples fall into 90th-100th percentile range.</h3>

<h2 id="overall-architecture-9">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/facial-embeds.png" alt="model architecture" /></p>

<h1 id="unsupervised-discovery-of-object-landmarks-as-structural-representationshttpsarxivorgpdf180404412pdf"><a href="https://arxiv.org/pdf/1804.04412.pdf">*14. Unsupervised Discovery of Object Landmarks as Structural Representations</a></h1>

<h2 id="method-10">Method</h2>

<h3 id="architecture-of-landmark-detector">Architecture of landmark detector</h3>

<ul>
  <li>$\mathbf{R} = hourglass_{l}(\mathbf{I};\theta_{l}) \in \mathbb{R}^{W \times H \times(K+1)}$: raw detection score map</li>
  <li>$\mathbf{D_{k}}(u,v)$: normalized $\mathbf{R}$ across the channels
=&gt; Taking $\mathbf{D_{k}}$ as a weighting map, $(x_{k}, y_{k})$ is the $k$-th landmark</li>
  <li>$l = [x_{1}, y_{1}, …, x_{k}, y_{k}]^{\top} = \text{landmark}(\mathbf{I};\theta_{l})$</li>
</ul>

<h3 id="visual-concept-of-landmarks">Visual concept of landmarks</h3>
<ul>
  <li>Concentration constraint: $\mathcal{L_{conc}} = 2\pi e (\sigma_{det, u}^{2}, \sigma_{det, v}^{2})$</li>
</ul>

<p>=&gt; $\bar{D_{k}}(u, v)=(1/WH)\mathcal{N}((u, v);(x_{k}, y_{k}), \sigma_{det}^{2}\mathbb{I})$</p>

<ul>
  <li>
    <p>Separation constraint: $\mathcal{L_{sep}} = \sum_{k \neq k’}^{1, …, K}\exp(-\frac{\vert\vert (x_{k’}, y_{k’}) - (x_{k}, y_{k}) \vert\vert ^{2}}{2\sigma_{sep}^{2}})$</p>
  </li>
  <li>
    <p>Equivariance constarint: $\mathcal{L_{equiv}} = \sum_{k=1}^{K} \vert\vert g(x_{k}’, y_{k}’) - (x_{k}, y_{k}) \vert\vert ^ {2}$</p>
  </li>
</ul>

<h3 id="local-latent-descriptors">Local latent descriptors</h3>

<p>$\mathbf{F} = hourglass_{f}(\mathbf{I};\theta_{f}) \in \mathbb{R}^{W \times H \times S}$</p>

<p>Then take $\mathbf{f_{k}}$ as inner product of $\bar{D_{k}}(u, v), \mathbf{F}(u, v)$ multiplied by landmark-specific operator $\mathbf{W_{k}}$</p>

<h3 id="landmark-based-decoder">Landmark-based decoder</h3>

<h3 id="total-loss-lambdareconlreconlambdaconclconclambdaseplseplambdaequivlequiv">Total Loss: $\lambda_{recon}L_{recon}+\lambda_{conc}L_{conc}+\lambda_{sep}L_{sep}+\lambda_{equiv}L_{equiv}$</h3>

<h2 id="overall-architecture-10">Overall Architecture</h2>

<p><img src="../../../../img/Unsupervised-Landmark/struct.png" alt="model architecture" /></p>

<h1 id="cross-domain-correspondence-learning-for-exemplar-based-image-translationhttpsarxivorgpdf200405571pdf"><a href="https://arxiv.org/pdf/2004.05571.pdf">15. Cross-domain Correspondence Learning for Exemplar-based Image Translation</a></h1>

</article>



<section class="tags">
  <strong>Tags:</strong> <a href="/tag/deep-learning">deep-learning</a>,&nbsp;<a href="/tag/unsupervised-learning">unsupervised-learning</a>
</section>



<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
  
</section>

	<section class="post-navigation">
		<span class="prev-post">
			
				<a href="/generative-models/2020/12/20/DCGAN.html">
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-left fa-stack-1x fa-inverse"></i>
					</span>
					<span class="page-number">DCGAN 리뷰</span>
				</a>
			
		</span>
		<span class="next-post">
			
				<a href="/generative-models/2020/12/22/from-Variational-Inference-to-VAE.html">
					<span class="page-number">from Variational Inference to VAE</span>
					<span class="fa-stack fa-lg">
						<i class="fa fa-square fa-stack-2x"></i>
						<i class="fa fa-angle-double-right fa-stack-1x fa-inverse"></i>
					</span>
				</a>
			
		</span>
	</section>




<section class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname = 'subong0508-github-io';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Data Vision</h3>

    <div class="site-navigation">
      
      <p><strong>Site Map</strong></p>
      <ul class="pages">
        
        
          <li class="nav-link"><a href="/about/">About</a>
        
        
        
        
        
        
        
        
        
          <li class="nav-link"><a href="/posts/">Posts</a>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:subong0508@naver.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">subong0508@naver.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/subong0508" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">subong0508</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">My personal blog for studying; mainly related to data
</p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.5/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/js/lightbox.min.js"></script>
<script>

$(document).ready(function() {

  // Syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });


});

</script>




  </body>

</html>
